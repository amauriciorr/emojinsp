{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logistic_regression_preprocess_embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NNWwwg_V8BV6",
        "DtuXsHxF8hgO",
        "sLkaHBPAioHf"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6omA1Kcjev_",
        "outputId": "082a6400-35bc-4e11-d6b2-18de70340bd3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gn_hEojLp9XH",
        "outputId": "69b535c9-9cbd-4588-e124-187f88063fc6"
      },
      "source": [
        "import gensim.models as gsm\n",
        "import gensim.downloader\n",
        "import pandas as pd\n",
        "import time\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sfus74UCYZh5"
      },
      "source": [
        "**Emoji2Vec Download**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzvLfnqOpN_Z"
      },
      "source": [
        "e2v = gsm.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/2021_NLU/emoji2vec.bin', binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72SXbLsK8Tyg",
        "outputId": "f2e0f0da-4a26-4869-ce86-53faf705b882"
      },
      "source": [
        "e2v.vector_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTxO6FnEpR_U",
        "outputId": "dfd0f6a2-3a40-4432-d6d0-ce042a8a739f"
      },
      "source": [
        "happy_vector = e2v['üòÇ']  \n",
        "happy_vector.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SyeCbyypR2p"
      },
      "source": [
        "# print(len(e2v.vocab))\n",
        "# print(e2v.vocab.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COtWbV2QYjF3"
      },
      "source": [
        "**Word2Vec Download**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3M0DFXRrpkdq",
        "outputId": "c79592d9-9e19-4448-8776-1c6f4e906111"
      },
      "source": [
        "word2vec = gensim.downloader.load('word2vec-google-news-300')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVcr51LL38lL"
      },
      "source": [
        "pickle.dump(word2vec, open('/content/drive/MyDrive/2021_NLU/data/full_data/word2vec.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yH7euci0mEU"
      },
      "source": [
        "with open('/content/drive/MyDrive/2021_NLU/data/full_data/word2vec.pkl', 'rb') as f:  \n",
        "    word2vec = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_owdfNn0ar6"
      },
      "source": [
        "## FULL DATA (SINGLE AND MULTI) - Download, Preprocess, Create Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjw34dNoqAQw"
      },
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/2021_NLU/data/full_data/emoji_nsp_dataset_train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39ly4uVw2anB"
      },
      "source": [
        "df_val = pd.read_csv('/content/drive/MyDrive/2021_NLU/data/full_data/emoji_nsp_dataset_valid.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gR8PpYFo99HG"
      },
      "source": [
        "df_test = pd.read_csv('/content/drive/MyDrive/2021_NLU/data/full_data/emoji_nsp_dataset_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "1TAAIGd9rBi_",
        "outputId": "399f4e8f-6e22-4d93-8cad-9630fae59f5b"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>tweets</th>\n",
              "      <th>emoji_sentence</th>\n",
              "      <th>follows?</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>50553</td>\n",
              "      <td>The dababy memes make no sense and that‚Äôs why ...</td>\n",
              "      <td>üò≠</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>74541</td>\n",
              "      <td>a year ago today i would be holding my breath ...</td>\n",
              "      <td>üòµ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>50992</td>\n",
              "      <td>I told my mama about how the music industry is...</td>\n",
              "      <td>üíØ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>95343</td>\n",
              "      <td>[USER] [USER] Thankyou guys</td>\n",
              "      <td>üíØ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>60555</td>\n",
              "      <td>You want new SUBS? Like Ô∏è Retweet Follow me  R...</td>\n",
              "      <td>üòÇüòÇ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  ... follows?\n",
              "0  50553  ...        1\n",
              "1  74541  ...        0\n",
              "2  50992  ...        1\n",
              "3  95343  ...        0\n",
              "4  60555  ...        0\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCl9_bZvrsQm",
        "outputId": "b1a991b0-d9c2-4283-d61e-2b61993b3a92"
      },
      "source": [
        "df_train.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "index             15540\n",
              "tweets            15540\n",
              "emoji_sentence    15540\n",
              "follows?          15540\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ebl2VyhL44Kl",
        "outputId": "b1832780-0391-4269-f79a-e8593ac03cea"
      },
      "source": [
        "df_train[\"emoji_sentence\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         üòé\n",
              "1         ü§≠\n",
              "2         ü•∫\n",
              "3         üôè\n",
              "4         üôÉ\n",
              "         ..\n",
              "15535     ü§∑\n",
              "15536     ü§£\n",
              "15537     ü•∫\n",
              "15538    üôåüç∫\n",
              "15539    üôÑüòã\n",
              "Name: emoji_sentence, Length: 15540, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaS2Ayhm6j6q",
        "outputId": "053d5ffc-60d2-4749-f42c-c9256fa753c0"
      },
      "source": [
        "df_train[\"emoji_sentence\"].isna()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        False\n",
              "1        False\n",
              "2        False\n",
              "3        False\n",
              "4        False\n",
              "         ...  \n",
              "14396    False\n",
              "14397    False\n",
              "14398    False\n",
              "14399    False\n",
              "14400    False\n",
              "Name: emoji_sentence, Length: 14401, dtype: bool"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzvzvJ2-nqm2",
        "outputId": "6a9b8f5c-b9d1-4ca6-adf5-80045d715ba9"
      },
      "source": [
        "print(len(df_train))\n",
        "print(len(df_val))\n",
        "print(len(df_test))\n",
        "\n",
        "df_train = df_train.dropna().drop_duplicates()\n",
        "df_val = df_val.dropna().drop_duplicates()\n",
        "df_test = df_test.dropna().drop_duplicates()\n",
        "\n",
        "print(len(df_train))\n",
        "print(len(df_val))\n",
        "print(len(df_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15540\n",
            "2199\n",
            "4438\n",
            "15540\n",
            "2199\n",
            "4438\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNWwwg_V8BV6"
      },
      "source": [
        "###Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrP-V85go6Ez"
      },
      "source": [
        "def make_lowercase(data, debug=False):\n",
        "\t'''\n",
        "\t- input: data - list of documents\n",
        "\t- output: data - list of documents after lowercasing everything\n",
        "\t'''\n",
        "\tif(debug):\n",
        "\t\tprint(\"data_sample out of \",len(data))\n",
        "\t\tprint(data[:sample_to_print])\n",
        "\tstart = time.time()\n",
        "\tdata = [i.lower() for i in data]\n",
        "\n",
        "\tend = time.time()\n",
        "\tprint('\\n       ##### Lowercasing Done! Time Taken - ',end-start)\n",
        "\treturn data                                                                       \n",
        "\n",
        "\n",
        "def punctuation_removal(data, debug=False):\n",
        "\t'''\n",
        "\t- input: data - list of documents\n",
        "\t- output: data - list of documents after removing punctuation\n",
        "\t'''\n",
        "\tif(debug):\n",
        "\t\tprint(\"data_sample out of \",len(data))\n",
        "\t\tprint(data[:sample_to_print])\n",
        "\tstart = time.time()\n",
        "\tdata = [i.translate(str.maketrans(string.punctuation,' '*len(string.punctuation))) for i in data]\n",
        "\tend = time.time()\n",
        "\tprint('\\n       ##### Punctuation removed! Time Taken - ',end-start)\n",
        "\treturn data\n",
        "\n",
        "def whitespace_removal(data, debug=False):\n",
        "\t'''\n",
        "\t- input: data - \n",
        "\t- output: data - \n",
        "\t'''\n",
        "\tif(debug):\n",
        "\t\tprint(\"data_sample out of \",len(data))\n",
        "\t\tprint(data[:sample_to_print])\n",
        "\tstart = time.time()\n",
        "\tdata = [' '.join(mystring.split()) for mystring in data]\n",
        "\t# data = [i.strip() for i in data]\n",
        "\tend = time.time()\n",
        "\tprint('\\n       ##### Whitespace removed! Time Taken - ',end-start)\n",
        "\treturn data\n",
        "\n",
        "# TOKENIZATION with NLTK\n",
        "def tokenization_nltk(data, debug=False):\n",
        "\t'''\n",
        "\t- input: data - \n",
        "\t- output: data - \n",
        "\t'''\n",
        "\tif(debug):\n",
        "\t\tprint(\"data_sample out of \",len(data))\n",
        "\t\tprint(data[:sample_to_print])\n",
        "\t# Using NLTK\n",
        "\tstart = time.time()\n",
        "\tdata = [nltk.word_tokenize(i) for i in data]\n",
        "\tend = time.time()\n",
        "\t# Using Spacy - Spacy takes too much time\n",
        "\t#data = [[token.text for token in nlp_spacy(i)] for i in data]\n",
        "\tprint('\\n       ##### Tokenization Done using NLTK! Time Taken - ', end-start)\n",
        "\treturn data\n",
        "\n",
        "# #used to search in nltk stop_words\n",
        "# def BinarySearch(a, x): \n",
        "# \ti = bisect_left(a, x) \n",
        "# \tif i != len(a) and a[i] == x:\n",
        "# \t\treturn i \n",
        "# \telse: \n",
        "# \t\treturn -1\n",
        "\n",
        "# def stopwords_removal(data, stop_words_nltk, debug=False):\n",
        "# \t'''\n",
        "# \t- input: data - \n",
        "# \t- output: data - \n",
        "# \t'''\n",
        "# \tif(debug):\n",
        "# \t\tprint(\"stopwords_removal_nltk data_sample out of \",len(data))\n",
        "# \t\tprint(data[:sample_to_print])\n",
        "# \t#using NLTK\n",
        "# \tstart = time.time()\n",
        "# \tdata = [[j for j in doc if (BinarySearch(stop_words_nltk,j)<0)] for doc in data]\n",
        "# \tdata = [[x for x in word if not (x.isdigit() or x[0] == '-' and x[1:].isdigit())] for word in data]\n",
        "# \tend = time.time()\n",
        "# \tprint('\\n       ##### Stopwords Removed using NLTK! Time Taken - ',end-start)\n",
        "# \treturn data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T0O29AYoGgT"
      },
      "source": [
        "def clean_text(sample, debug=False):\n",
        "  '''\n",
        "  sample should be a list of documents\n",
        "  '''\n",
        "\n",
        "\n",
        "  # sample = remove_string_with_nonASCII(sample)\n",
        "  # if debug:\n",
        "  #   print(sample[:2])\n",
        "\n",
        "  # sample = preprocess_tweet_text(sample)\n",
        "  # if debug:\n",
        "  #   print(sample[:2])\n",
        "    \n",
        "  sample = make_lowercase(sample)\n",
        "  if debug:\n",
        "    print(sample[:2])\n",
        "\n",
        "  sample = punctuation_removal(sample)\n",
        "  if debug:\n",
        "    print(sample[:2])\n",
        "\n",
        "  sample = whitespace_removal(sample)\n",
        "  if debug:\n",
        "    print(sample[:2])\n",
        "\n",
        "  sample = tokenization_nltk(sample)\n",
        "  if debug:\n",
        "    print(sample[:2])\n",
        "\n",
        "  # sample = tokenization_spacy(sample)\n",
        "  # if debug:\n",
        "  #   print(sample[:2])\n",
        "\n",
        "  # sample = lemmatization_tokenization_spacy(sample)\n",
        "  # if debug:\n",
        "    # print(sample[:2])\n",
        "\n",
        "  # sample = stopwords_removal(sample, stop_words_nltk)\n",
        "  # if debug:\n",
        "  #   print(sample[:2])\n",
        "\n",
        "  # sample = make_bigrams_gensim(sample, bigrams_min_count=10, bigrams_threshold=10) #params from gensim\n",
        "  # if debug:\n",
        "  #   print(sample[:2])\n",
        "\n",
        "  sample_normal = [' '.join(i) for i in sample]\n",
        "  # Sample tokenized is used for Word2Vec\n",
        "\n",
        "  return sample, sample_normal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iddyM7tMqGa0",
        "outputId": "275907a0-80fe-40be-fe3e-6f4defd3a8e1"
      },
      "source": [
        "# sample_tokenized, sample_normal = clean_text(sample)\n",
        "x_train_tokenized, x_train_normal = clean_text(df_train['tweets'].values)\n",
        "x_val_tokenized, x_val_normal = clean_text(df_val['tweets'].values)\n",
        "x_test_tokenized, x_test_normal = clean_text(df_test['tweets'].values)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "       ##### Lowercasing Done! Time Taken -  0.012780904769897461\n",
            "\n",
            "       ##### Punctuation removed! Time Taken -  0.09380507469177246\n",
            "\n",
            "       ##### Whitespace removed! Time Taken -  0.02262401580810547\n",
            "\n",
            "       ##### Tokenization Done using NLTK! Time Taken -  2.0198376178741455\n",
            "\n",
            "       ##### Lowercasing Done! Time Taken -  0.00092315673828125\n",
            "\n",
            "       ##### Punctuation removed! Time Taken -  0.013142824172973633\n",
            "\n",
            "       ##### Whitespace removed! Time Taken -  0.003053903579711914\n",
            "\n",
            "       ##### Tokenization Done using NLTK! Time Taken -  0.2526061534881592\n",
            "\n",
            "       ##### Lowercasing Done! Time Taken -  0.0018680095672607422\n",
            "\n",
            "       ##### Punctuation removed! Time Taken -  0.02589106559753418\n",
            "\n",
            "       ##### Whitespace removed! Time Taken -  0.006514072418212891\n",
            "\n",
            "       ##### Tokenization Done using NLTK! Time Taken -  0.5394716262817383\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mFlX8HC8Nj2"
      },
      "source": [
        "y_train = df_train['follows?'].values\n",
        "y_val = df_val['follows?'].values\n",
        "y_test = df_test['follows?'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szipRmZv8Ntj"
      },
      "source": [
        "#remove empty values for train\n",
        "y_train_2 = []\n",
        "x_train_tokenized_2 = []\n",
        "x_train_normal_2 = []\n",
        "empty_indices_train = []\n",
        "\n",
        "for i in range(len(x_train_tokenized)):\n",
        "  if len(x_train_tokenized[i])==0:\n",
        "    empty_indices_train.append(i)\n",
        "  else:\n",
        "    x_train_tokenized_2.append(x_train_tokenized[i])\n",
        "    x_train_normal_2.append(x_train_normal[i])\n",
        "    y_train_2.append(y_train[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcTijrpg9A1Z"
      },
      "source": [
        "#remove empty values for val\n",
        "y_val_2 = []\n",
        "x_val_tokenized_2 = []\n",
        "x_val_normal_2 = []\n",
        "empty_indices_val = []\n",
        "\n",
        "for i in range(len(x_val_tokenized)):\n",
        "  if len(x_val_tokenized[i])==0:\n",
        "    empty_indices_val.append(i)\n",
        "  else:\n",
        "    x_val_tokenized_2.append(x_val_tokenized[i])\n",
        "    x_val_normal_2.append(x_val_normal[i])\n",
        "    y_val_2.append(y_val[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yqUX_iR9A5W"
      },
      "source": [
        "#remove empty values for test\n",
        "y_test_2 = []\n",
        "x_test_tokenized_2 = []\n",
        "x_test_normal_2 = []\n",
        "empty_indices_test = []\n",
        "\n",
        "for i in range(len(x_test_tokenized)):\n",
        "  if len(x_test_tokenized[i])==0:\n",
        "    empty_indices_test.append(i)\n",
        "  else:\n",
        "    x_test_tokenized_2.append(x_test_tokenized[i])\n",
        "    x_test_normal_2.append(x_test_normal[i])\n",
        "    y_test_2.append(y_test[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdSWeXeQ9BB6",
        "outputId": "cb9341e6-abe7-4e23-ab5f-76fe795e15cb"
      },
      "source": [
        "print(empty_indices_train)\n",
        "print(len(empty_indices_train))\n",
        "print(len(x_train_tokenized))\n",
        "print(len(x_train_tokenized_2))\n",
        "print(len(x_train_normal_2))\n",
        "print(len(y_train_2))\n",
        "\n",
        "print(empty_indices_val)\n",
        "print(len(empty_indices_val))\n",
        "print(len(x_val_tokenized))\n",
        "print(len(x_val_tokenized_2))\n",
        "print(len(x_val_normal_2))\n",
        "print(len(y_val_2))\n",
        "\n",
        "print(empty_indices_test)\n",
        "print(len(empty_indices_test))\n",
        "print(len(x_test_tokenized))\n",
        "print(len(x_test_tokenized_2))\n",
        "print(len(x_test_normal_2))\n",
        "print(len(y_test_2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "0\n",
            "15540\n",
            "15540\n",
            "15540\n",
            "15540\n",
            "[]\n",
            "0\n",
            "2199\n",
            "2199\n",
            "2199\n",
            "2199\n",
            "[]\n",
            "0\n",
            "4438\n",
            "4438\n",
            "4438\n",
            "4438\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhFYPLWu8HNA"
      },
      "source": [
        "### Save tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dyp_54CI9BF-"
      },
      "source": [
        "# Saving the objects:\n",
        "with open('/content/drive/MyDrive/2021_NLU/data/full_data/full_data.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
        "    pickle.dump([x_train_tokenized, x_train_normal, x_val_tokenized, x_val_normal, x_test_tokenized, x_test_normal, empty_indices_train, x_train_tokenized_2, \\\n",
        "                 x_train_normal_2, empty_indices_val, x_val_tokenized_2, x_val_normal_2, y_train, y_val, y_train_2, y_val_2, y_test, y_test_2,\\\n",
        "                 empty_indices_test, x_test_normal_2, x_test_tokenized_2 ], f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lIVgsadxKLA"
      },
      "source": [
        "with open('/content/drive/MyDrive/2021_NLU/data/full_data/full_data.pkl', 'rb') as f:  \n",
        "    x_train_tokenized, x_train_normal, x_val_tokenized, x_val_normal, x_test_tokenized, x_test_normal, empty_indices_train, x_train_tokenized_2, \\\n",
        "                 x_train_normal_2, empty_indices_val, x_val_tokenized_2, x_val_normal_2, y_train, y_val, y_train_2, y_val_2, y_test, y_test_2,\\\n",
        "                 empty_indices_test, x_test_normal_2, x_test_tokenized_2 = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38nUJAfi9NXB"
      },
      "source": [
        "# def create_df(xdata, emojidata, ydata):\n",
        "#   temp=[\" \".join(i) for i in xdata]\n",
        "#   df_new = pd.DataFrame(temp)\n",
        "#   df_new[\"Emoji\"] = emojidata\n",
        "#   df_new[\"Target\"] = ydata\n",
        "\n",
        "#   df_new.columns = [\"Tweet\", \"Emoji\", \"Target\"]\n",
        "#   return df_new\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIS_kfnY9NbN"
      },
      "source": [
        "# df_new_val = create_df(x_val_tokenized_2,  y_val_2)\n",
        "# df_new_train = create_df(x_train_tokenized_2, y_train_2)\n",
        "# df_new_test = create_df(x_test_tokenized_2, df_test['emoji_sentence'], y_test_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj3WZZ_I4uK7"
      },
      "source": [
        "# df_new_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc1JhRi9i1iF",
        "outputId": "27a45dff-24db-4c07-ba2e-e512484b7e84"
      },
      "source": [
        "df_test['tokenized_tweets'] = x_test_tokenized\n",
        "df_test['tokenized_len'] = df_test['tokenized_tweets'].apply(lambda x: len(x))\n",
        "print(df_test['tokenized_len'].mean())\n",
        "print(df_test['tokenized_len'].median())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14.242000901306895\n",
            "10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9Gvk_ol9UY5"
      },
      "source": [
        "# # Saving the objects:\n",
        "# with open('/content/drive/MyDrive/2021_NLU/data/full_data/df_new.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
        "#     pickle.dump([df_new_val, df_new_train, df_new_test], f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKDpl79C9UcJ"
      },
      "source": [
        "# with open('/content/drive/MyDrive/2020 NLP/Project/df_new.pkl', 'rb') as f:  \n",
        "#     df_new_val, df_new_train, df_new_test = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtuXsHxF8hgO"
      },
      "source": [
        "### Create Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQmEFUlC9gU5"
      },
      "source": [
        "def convert_word2vec(model, corpus, strategy):\n",
        "  # return [[model[token] for token in sentence] for sentence in corpus]\n",
        "  output = []\n",
        "  for sentence in corpus:\n",
        "    vector_ = np.zeros(model.vector_size)\n",
        "    for token in sentence:\n",
        "      try:\n",
        "        token_vector = model[token]\n",
        "        vector_ = vector_ + token_vector\n",
        "      except:\n",
        "        vector_ = vector_ + np.zeros(model.vector_size)\n",
        "    if strategy=='mean':\n",
        "      vector_ = vector_/len(sentence)\n",
        "    elif strategy=='add':\n",
        "      pass\n",
        "    output.append(vector_)\n",
        "  # output is a list\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIHpx-MO9gZs"
      },
      "source": [
        "X_train_w2vec = convert_word2vec(word2vec, x_train_tokenized_2, strategy='mean')\n",
        "X_val_w2vec = convert_word2vec(word2vec, x_val_tokenized_2, strategy='mean')\n",
        "X_test_w2vec = convert_word2vec(word2vec, x_test_tokenized_2, strategy='mean')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNA44FrAzOER"
      },
      "source": [
        "# list(df_train['emoji_sentence'][2])\n",
        "df_train['emoji_sentence_list'] = df_train['emoji_sentence'].apply(lambda x: list(x))\n",
        "emoji_corpus_train = df_train['emoji_sentence_list'].values\n",
        "\n",
        "df_val['emoji_sentence_list'] = df_val['emoji_sentence'].apply(lambda x: list(x))\n",
        "emoji_corpus_val = df_val['emoji_sentence_list'].values\n",
        "\n",
        "df_test['emoji_sentence_list'] = df_test['emoji_sentence'].apply(lambda x: list(x))\n",
        "emoji_corpus_test = df_test['emoji_sentence_list'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0wjcIlwDJMw"
      },
      "source": [
        "def convert_emoji2vec(model, corpus, strategy):\n",
        "  # return [[model[token] for token in sentence] for sentence in corpus]\n",
        "  output = []\n",
        "  for emojis in corpus:\n",
        "    vector_ = np.zeros(model.vector_size)\n",
        "    for emoji in emojis:\n",
        "      try:\n",
        "        token_vector = model[emoji]\n",
        "        vector_ = vector_ + token_vector\n",
        "      except:\n",
        "        vector_ = vector_ + np.zeros(model.vector_size)\n",
        "    if strategy=='mean':\n",
        "      vector_ = vector_/len(emojis)\n",
        "    elif strategy=='add':\n",
        "      pass\n",
        "    output.append(vector_)\n",
        "  # output is a list\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqWMsSTD9gnq"
      },
      "source": [
        "X_train_e2vec = convert_emoji2vec(e2v, emoji_corpus_train, strategy='mean')\n",
        "X_val_e2vec = convert_emoji2vec(e2v, emoji_corpus_val, strategy='mean')\n",
        "X_test_e2vec = convert_emoji2vec(e2v, emoji_corpus_test, strategy='mean')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5xk3GbOACeb",
        "outputId": "376d85ea-ba16-4a6a-8f8c-1806176ef82e"
      },
      "source": [
        "print(len(X_train_w2vec))\n",
        "print(len(X_train_w2vec[4]))\n",
        "\n",
        "print(len(X_val_w2vec))\n",
        "print(len(X_val_w2vec[4]))\n",
        "\n",
        "print(len(X_test_w2vec))\n",
        "print(len(X_test_w2vec[4]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15540\n",
            "300\n",
            "2199\n",
            "300\n",
            "4438\n",
            "300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33EYEGrA_cti",
        "outputId": "77ed942c-3e85-4b56-c15d-73675b3ff88e"
      },
      "source": [
        "print(len(X_train_e2vec))\n",
        "print(len(X_train_e2vec[4]))\n",
        "\n",
        "print(len(X_val_e2vec))\n",
        "print(len(X_val_e2vec[4]))\n",
        "\n",
        "print(len(X_test_e2vec))\n",
        "print(len(X_test_e2vec[4]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15540\n",
            "300\n",
            "2199\n",
            "300\n",
            "4438\n",
            "300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmQCEf5Ig3ZB"
      },
      "source": [
        "#### Averaged Vectors "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kRpTqbjAYxB"
      },
      "source": [
        "X_train_vec = (np.array(X_train_w2vec) + np.array(X_train_e2vec))/2\n",
        "X_val_vec = (np.array(X_val_w2vec) + np.array(X_val_e2vec))/2\n",
        "X_test_vec = (np.array(X_test_w2vec) + np.array(X_test_e2vec))/2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXcy1curADf_",
        "outputId": "ce0614c7-9069-434c-959a-4d685572458c"
      },
      "source": [
        "X_train_w2vec[0][:4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.04621582, -0.00656535,  0.04308268,  0.0743042 ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mf71tBhPA4Fw",
        "outputId": "a55e5076-ad81-48c6-82e3-b38c2c0cde06"
      },
      "source": [
        "X_train_e2vec[0][:4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.03221022,  0.03802984, -0.00126745,  0.07279918])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhW5xZdmA4KQ",
        "outputId": "e1aa2d0f-0a59-4719-939b-0d5a294ca9ea"
      },
      "source": [
        "X_train_vec[0][:4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.03921302, 0.01573225, 0.02090762, 0.07355169])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKrkAztPBO-t",
        "outputId": "6db4cacb-fa27-4fde-c832-ae0281058eda"
      },
      "source": [
        "len(X_train_vec)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15540"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sODMNoNOhEZt"
      },
      "source": [
        "#### Concantenated Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1WbET1chHtn"
      },
      "source": [
        "X_train_vec_concat = np.concatenate((X_train_w2vec, X_train_e2vec), axis=1) \n",
        "X_val_vec_concat  = np.concatenate((X_val_w2vec, X_val_e2vec), axis=1) \n",
        "X_test_vec_concat  = np.concatenate((X_test_w2vec, X_test_e2vec), axis=1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TurUa1-xaHlA",
        "outputId": "3523c11d-8231-4394-9390-2d7713c252a3"
      },
      "source": [
        "print(len(X_train_w2vec))\n",
        "print(len(X_train_e2vec))\n",
        "print(len(X_train_vec_concat))\n",
        "print(len(X_train_w2vec[4]))\n",
        "print(len(X_train_vec_concat[4]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15540\n",
            "15540\n",
            "15540\n",
            "300\n",
            "600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "He-E1rm5Z6v6",
        "outputId": "9a8690ed-c7fc-49b0-e0a7-4ef12f135584"
      },
      "source": [
        "len(X_train_vec_concat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15540"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mbr_jNujjRUQ",
        "outputId": "8c2143be-cbb7-440f-ad54-0ea0b764c64e"
      },
      "source": [
        "X_train_vec_concat.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15540, 600)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftrgTt0h8Z7R"
      },
      "source": [
        "### Save Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j16u5CJuzldt"
      },
      "source": [
        "# Saving the objects:\n",
        "with open('/content/drive/MyDrive/2021_NLU/data/full_data/full_vec.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
        "    pickle.dump([X_train_vec, X_val_vec, X_test_vec], f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwUDi7-OzlhL"
      },
      "source": [
        "with open('/content/drive/MyDrive/2021_NLU/data/full_data/full_vec.pkl', 'rb') as f:  \n",
        "    X_train_vec, X_val_vec, X_test_vec = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIlaP0o8huEP"
      },
      "source": [
        "# Saving the objects:\n",
        "with open('/content/drive/MyDrive/2021_NLU/data/full_data/full_vec_concat.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
        "    pickle.dump([X_train_vec_concat, X_val_vec_concat, X_test_vec_concat], f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpYchOBhhuIX"
      },
      "source": [
        "with open('/content/drive/MyDrive/2021_NLU/data/full_data/full_vec.pkl', 'rb') as f:  \n",
        "    X_train_vec_concat, X_val_vec_concat, X_test_vec_concat = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SehXB2kF0ljA"
      },
      "source": [
        "## (ONLY) SINGLE EMOJI - Download, Preprocess, Create Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBfMT8ND0ob2"
      },
      "source": [
        "df_sing_train = pd.read_csv('/content/drive/MyDrive/2021_NLU/data/single_emoji/emoji_nsp_dataset_single_emoji_train.csv')\n",
        "df_sing_val = pd.read_csv('/content/drive/MyDrive/2021_NLU/data/single_emoji/emoji_nsp_dataset_single_emoji_valid.csv')\n",
        "df_sing_test = pd.read_csv('/content/drive/MyDrive/2021_NLU/data/single_emoji/emoji_nsp_dataset_single_emoji_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "b4vjIbWT0ofB",
        "outputId": "99e42f27-7cd2-46ff-d43d-25d0d7d95653"
      },
      "source": [
        "df_sing_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>tweets</th>\n",
              "      <th>emoji_sentence</th>\n",
              "      <th>follows?</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>67113</td>\n",
              "      <td>Craving Black Cake</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>35900</td>\n",
              "      <td>y‚Äôall i was kidding, pls don‚Äôt attack me</td>\n",
              "      <td>üò≠</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1045</td>\n",
              "      <td>omg bye so true</td>\n",
              "      <td>üò≠</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11764</td>\n",
              "      <td>Nooo I‚Äôm at the end of the og Futurama eps</td>\n",
              "      <td>üò¢</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1642</td>\n",
              "      <td>[USER] [USER] [USER] [USER] Not worth wasting ...</td>\n",
              "      <td>üòç</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  ... follows?\n",
              "0  67113  ...        0\n",
              "1  35900  ...        1\n",
              "2   1045  ...        1\n",
              "3  11764  ...        1\n",
              "4   1642  ...        1\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4I6fFWe0oj6",
        "outputId": "8f5480ae-21c3-409a-ec33-a64d5b563cdf"
      },
      "source": [
        "print(len(df_sing_train))\n",
        "print(len(df_sing_val))\n",
        "print(len(df_sing_test))\n",
        "\n",
        "df_sing_train = df_sing_train.dropna().drop_duplicates()\n",
        "df_sing_val = df_sing_val.dropna().drop_duplicates()\n",
        "df_sing_test = df_sing_test.dropna().drop_duplicates()\n",
        "\n",
        "print(len(df_sing_train))\n",
        "print(len(df_sing_val))\n",
        "print(len(df_sing_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15015\n",
            "2153\n",
            "4341\n",
            "15015\n",
            "2153\n",
            "4341\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGCW8hdc0omx",
        "outputId": "25aea3cd-7cb8-4608-da32-978f40d28a3b"
      },
      "source": [
        "# sample_tokenized, sample_normal = clean_text(sample)\n",
        "x_train_sing_tokenized, x_train_sing_normal = clean_text(df_sing_train['tweets'].values)\n",
        "x_val_sing_tokenized, x_val_sing_normal = clean_text(df_sing_val['tweets'].values)\n",
        "x_test_sing_tokenized, x_test_sing_normal = clean_text(df_sing_test['tweets'].values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "       ##### Lowercasing Done! Time Taken -  0.011325836181640625\n",
            "\n",
            "       ##### Punctuation removed! Time Taken -  0.07462406158447266\n",
            "\n",
            "       ##### Whitespace removed! Time Taken -  0.020409584045410156\n",
            "\n",
            "       ##### Tokenization Done using NLTK! Time Taken -  1.7219054698944092\n",
            "\n",
            "       ##### Lowercasing Done! Time Taken -  0.0008127689361572266\n",
            "\n",
            "       ##### Punctuation removed! Time Taken -  0.009940624237060547\n",
            "\n",
            "       ##### Whitespace removed! Time Taken -  0.0026712417602539062\n",
            "\n",
            "       ##### Tokenization Done using NLTK! Time Taken -  0.24480819702148438\n",
            "\n",
            "       ##### Lowercasing Done! Time Taken -  0.0017445087432861328\n",
            "\n",
            "       ##### Punctuation removed! Time Taken -  0.023852109909057617\n",
            "\n",
            "       ##### Whitespace removed! Time Taken -  0.006357908248901367\n",
            "\n",
            "       ##### Tokenization Done using NLTK! Time Taken -  0.4971282482147217\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVdQHYmk0orG"
      },
      "source": [
        "y_sing_train = df_sing_train['follows?'].values\n",
        "y_sing_val = df_sing_val['follows?'].values\n",
        "y_sing_test = df_sing_test['follows?'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_tTjjSL3nGe"
      },
      "source": [
        "#remove empty values for train\n",
        "y_train_sing_2 = []\n",
        "x_train_sing_tokenized_2 = []\n",
        "x_train_sing_normal_2 = []\n",
        "empty_indices_sing_train = []\n",
        "\n",
        "for i in range(len(x_train_sing_tokenized)):\n",
        "  if len(x_train_sing_tokenized[i])==0:\n",
        "    empty_indices_sing_train.append(i)\n",
        "  else:\n",
        "    x_train_sing_tokenized_2.append(x_train_sing_tokenized[i])\n",
        "    x_train_sing_normal_2.append(x_train_sing_normal[i])\n",
        "    y_train_sing_2.append(y_sing_train[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blwEBMvm4Cry"
      },
      "source": [
        "#remove empty values for val\n",
        "y_val_sing_2 = []\n",
        "x_val_sing_tokenized_2 = []\n",
        "x_val_sing_normal_2 = []\n",
        "empty_indices_sing_val = []\n",
        "\n",
        "for i in range(len(x_val_sing_tokenized)):\n",
        "  if len(x_val_sing_tokenized[i])==0:\n",
        "    empty_indices_sing_val.append(i)\n",
        "  else:\n",
        "    x_val_sing_tokenized_2.append(x_val_sing_tokenized[i])\n",
        "    x_val_sing_normal_2.append(x_val_sing_normal[i])\n",
        "    y_val_sing_2.append(y_sing_val[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etNPNC7G4Cu6"
      },
      "source": [
        "#remove empty values for test\n",
        "y_test_sing_2 = []\n",
        "x_test_sing_tokenized_2 = []\n",
        "x_test_sing_normal_2 = []\n",
        "empty_indices_sing_test = []\n",
        "\n",
        "for i in range(len(x_test_sing_tokenized)):\n",
        "  if len(x_test_sing_tokenized[i])==0:\n",
        "    empty_indices_sing_test.append(i)\n",
        "  else:\n",
        "    x_test_sing_tokenized_2.append(x_test_sing_tokenized[i])\n",
        "    x_test_sing_normal_2.append(x_test_sing_normal[i])\n",
        "    y_test_sing_2.append(y_sing_test[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daxAEWZN3nJ9",
        "outputId": "aa25807f-3e2b-4df1-a560-5ffa4b1f52d8"
      },
      "source": [
        "print(empty_indices_sing_train)\n",
        "print(len(empty_indices_sing_train))\n",
        "print(len(x_train_sing_tokenized))\n",
        "print(len(x_train_sing_tokenized_2))\n",
        "print(len(x_train_sing_normal_2))\n",
        "print(len(y_train_sing_2))\n",
        "\n",
        "print(empty_indices_sing_val)\n",
        "print(len(empty_indices_sing_val))\n",
        "print(len(x_val_sing_tokenized))\n",
        "print(len(x_val_sing_tokenized_2))\n",
        "print(len(x_val_sing_normal_2))\n",
        "print(len(y_val_sing_2))\n",
        "\n",
        "print(empty_indices_sing_test)\n",
        "print(len(empty_indices_sing_test))\n",
        "print(len(x_test_sing_tokenized))\n",
        "print(len(x_test_sing_tokenized_2))\n",
        "print(len(x_test_sing_normal_2))\n",
        "print(len(y_test_sing_2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "0\n",
            "15015\n",
            "15015\n",
            "15015\n",
            "15015\n",
            "[]\n",
            "0\n",
            "2153\n",
            "2153\n",
            "2153\n",
            "2153\n",
            "[]\n",
            "0\n",
            "4341\n",
            "4341\n",
            "4341\n",
            "4341\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehZZMjTd3nNi"
      },
      "source": [
        "# Saving the objects:\n",
        "with open('/content/drive/MyDrive/2021_NLU/data/single_emoji/single_emoji.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
        "    pickle.dump([x_train_sing_tokenized, x_train_sing_normal, x_val_sing_tokenized, x_val_sing_normal, x_test_sing_tokenized, x_test_sing_normal, empty_indices_sing_train, x_train_sing_tokenized_2, \\\n",
        "                 x_train_sing_normal_2, empty_indices_sing_val, x_val_sing_tokenized_2, x_val_sing_normal_2, y_sing_train, y_sing_val, y_train_sing_2, y_val_sing_2, y_sing_test, y_test_sing_2,\\\n",
        "                 empty_indices_sing_test, x_test_sing_normal_2, x_test_sing_tokenized_2 ], f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pfu3aE2T7H41"
      },
      "source": [
        "with open('/content/drive/MyDrive/2021_NLU/data/single_emoji/single_emoji.pkl', 'rb') as f:  \n",
        "    x_train_sing_tokenized, x_train_sing_normal, x_val_sing_tokenized, x_val_sing_normal, x_test_sing_tokenized, x_test_sing_normal, empty_indices_sing_train, x_train_sing_tokenized_2, \\\n",
        "                 x_train_sing_normal_2, empty_indices_sing_val, x_val_sing_tokenized_2, x_val_sing_normal_2, y_sing_train, y_sing_val, y_train_sing_2, y_val_sing_2, y_sing_test, y_test_sing_2,\\\n",
        "                 empty_indices_sing_test, x_test_sing_normal_2, x_test_sing_tokenized_2 = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbZ32AnX9sXR",
        "outputId": "c6dc079a-29f8-4109-c764-9bd2bcfc8f7b"
      },
      "source": [
        "len(x_train_sing_tokenized_2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15015"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCKzMv5F66aY"
      },
      "source": [
        "X_train_sing_w2vec = convert_word2vec(word2vec, x_train_sing_tokenized_2, strategy='mean')\n",
        "X_val_sing_w2vec = convert_word2vec(word2vec, x_val_sing_tokenized_2, strategy='mean')\n",
        "X_test_sing_w2vec = convert_word2vec(word2vec, x_test_sing_tokenized_2, strategy='mean')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-eC3wmv66fX"
      },
      "source": [
        "# list(df_train['emoji_sentence'][2])\n",
        "df_sing_train['emoji_sentence_list'] = df_sing_train['emoji_sentence'].apply(lambda x: list(x))\n",
        "emoji_corpus_sing_train = df_sing_train['emoji_sentence_list'].values\n",
        "\n",
        "df_sing_val['emoji_sentence_list'] = df_sing_val['emoji_sentence'].apply(lambda x: list(x))\n",
        "emoji_corpus_sing_val = df_sing_val['emoji_sentence_list'].values\n",
        "\n",
        "df_sing_test['emoji_sentence_list'] = df_sing_test['emoji_sentence'].apply(lambda x: list(x))\n",
        "emoji_corpus_sing_test = df_sing_test['emoji_sentence_list'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkZOFmbZ66j9"
      },
      "source": [
        "X_train_sing_e2vec = convert_emoji2vec(e2v, emoji_corpus_sing_train, strategy='mean')\n",
        "X_val_sing_e2vec = convert_emoji2vec(e2v, emoji_corpus_sing_val, strategy='mean')\n",
        "X_test_sing_e2vec = convert_emoji2vec(e2v, emoji_corpus_sing_test, strategy='mean')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5B_8QeZ8S91",
        "outputId": "7314a22e-1956-44a8-e62b-565896987b8e"
      },
      "source": [
        "print(len(X_train_sing_w2vec))\n",
        "print(len(X_train_sing_w2vec[4]))\n",
        "\n",
        "print(len(X_val_sing_w2vec))\n",
        "print(len(X_val_sing_w2vec[4]))\n",
        "\n",
        "print(len(X_test_sing_w2vec))\n",
        "print(len(X_test_sing_w2vec[4]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15015\n",
            "300\n",
            "2153\n",
            "300\n",
            "4341\n",
            "300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Al3zV9Qe8TB0",
        "outputId": "72143b7c-18b1-49a4-c53f-229be21309ba"
      },
      "source": [
        "print(len(X_train_sing_e2vec))\n",
        "print(len(X_train_sing_e2vec[4]))\n",
        "\n",
        "print(len(X_val_sing_e2vec))\n",
        "print(len(X_val_sing_e2vec[4]))\n",
        "\n",
        "print(len(X_test_sing_e2vec))\n",
        "print(len(X_test_sing_e2vec[4]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15015\n",
            "300\n",
            "2153\n",
            "300\n",
            "4341\n",
            "300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yB8nhFLGbj7p"
      },
      "source": [
        "#### Averaged Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krigRs6z87So"
      },
      "source": [
        "X_train_sing_vec = (np.array(X_train_sing_w2vec) + np.array(X_train_sing_e2vec))/2\n",
        "X_val_sing_vec = (np.array(X_val_sing_w2vec) + np.array(X_val_sing_e2vec))/2\n",
        "X_test_sing_vec = (np.array(X_test_sing_w2vec) + np.array(X_test_sing_e2vec))/2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi-L4mHI87XZ"
      },
      "source": [
        "# Saving the objects:\n",
        "with open('/content/drive/MyDrive/2021_NLU/data/single_emoji/single_emoji_vec.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
        "    pickle.dump([X_train_sing_vec, X_val_sing_vec, X_test_sing_vec], f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd7GQKsE87e4"
      },
      "source": [
        "with open('/content/drive/MyDrive/2021_NLU/data/single_emoji/single_emoji_vec.pkl', 'rb') as f:  \n",
        "    X_train_sing_vec, X_val_sing_vec, X_test_sing_vec = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WwMp9oobnc3"
      },
      "source": [
        "#### Concatenated Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcKY4UsN0ous"
      },
      "source": [
        "X_train_sing_vec_concat = np.concatenate((X_train_sing_w2vec, X_train_sing_e2vec), axis=1) \n",
        "X_val_sing_vec_concat  = np.concatenate((X_val_sing_w2vec, X_val_sing_e2vec), axis=1) \n",
        "X_test_sing_vec_concat  = np.concatenate((X_test_sing_w2vec, X_test_sing_e2vec), axis=1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPJ02Ci4brbq"
      },
      "source": [
        "# Saving the objects:\n",
        "with open('/content/drive/MyDrive/2021_NLU/data/single_emoji/single_emoji_vec_concat.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
        "    pickle.dump([X_train_sing_vec_concat, X_val_sing_vec_concat, X_test_sing_vec_concat], f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lkXVR32brgd"
      },
      "source": [
        "with open('/content/drive/MyDrive/2021_NLU/data/single_emoji/single_emoji_vec_concat.pkl', 'rb') as f:  \n",
        "    X_train_sing_vec_concat, X_val_sing_vec_concat, X_test_sing_vec_concat = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWED9-T91BBc"
      },
      "source": [
        "## (ONLY) MULTI EMOJIS WITH REPEATS - Download, Preprocess, Create Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dClnKTpX1HD4"
      },
      "source": [
        "df_mul_repeat_train = pd.read_csv('/content/drive/MyDrive/2021_NLU/data/multi_emoji/emoji_nsp_dataset_multi_emoji_train.csv')\n",
        "df_mul_repeat_val = pd.read_csv('/content/drive/MyDrive/2021_NLU/data/multi_emoji/emoji_nsp_dataset_multi_emoji_valid.csv')\n",
        "df_mul_repeat_test = pd.read_csv('/content/drive/MyDrive/2021_NLU/data/multi_emoji/emoji_nsp_dataset_multi_emoji_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "TIGjSwgy1HGz",
        "outputId": "b2c18d57-03d0-42d5-d159-c2eabbdeab65"
      },
      "source": [
        "df_mul_repeat_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>tweets</th>\n",
              "      <th>emoji_sentence</th>\n",
              "      <th>follows?</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6704</td>\n",
              "      <td>good morning [USER] you woke up so early  anyw...</td>\n",
              "      <td>üò∂üòäüíö</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10651</td>\n",
              "      <td>MVP  4x Scoring champ  3x All NBA FIRST TEAM 3...</td>\n",
              "      <td>‚úÖ‚úÖ‚úÖ‚úÖ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6345</td>\n",
              "      <td>[USER] Your tears baby boy ...Sleep sad</td>\n",
              "      <td>üåöüåöüëå</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5268</td>\n",
              "      <td>15 php ---&amp;gt; Follow [USER] ---&amp;gt; RT &amp;amp; ...</td>\n",
              "      <td>üíóüìå</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5897</td>\n",
              "      <td>this emoji looks like sia</td>\n",
              "      <td>üò≠üçôüçôüçôüçôüçôüçôüçôüçôüçôüçôüçôüçôüçôüçôüçôüçôüçôüçôüçôüçôüçô</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  ... follows?\n",
              "0   6704  ...        1\n",
              "1  10651  ...        1\n",
              "2   6345  ...        1\n",
              "3   5268  ...        1\n",
              "4   5897  ...        1\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHdzG_ny1HJR",
        "outputId": "16759c9e-0096-4f7a-d565-dc66625f22f6"
      },
      "source": [
        "print(len(df_mul_repeat_train))\n",
        "print(len(df_mul_repeat_val))\n",
        "print(len(df_mul_repeat_test))\n",
        "\n",
        "df_mul_repeat_train = df_mul_repeat_train.dropna().drop_duplicates()\n",
        "df_mul_repeat_val = df_mul_repeat_val.dropna().drop_duplicates()\n",
        "df_mul_repeat_test = df_mul_repeat_test.dropna().drop_duplicates()\n",
        "\n",
        "print(len(df_mul_repeat_train))\n",
        "print(len(df_mul_repeat_val))\n",
        "print(len(df_mul_repeat_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16801\n",
            "2401\n",
            "4800\n",
            "16801\n",
            "2401\n",
            "4800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9fJOj3M1HMr",
        "outputId": "e4d98d84-a632-48af-dcad-585128ae134f"
      },
      "source": [
        "# sample_tokenized, sample_normal = clean_text(sample)\n",
        "x_train_mul_repeat_tokenized, x_train_mul_repeat_normal = clean_text(df_mul_repeat_train['tweets'].values)\n",
        "x_val_mul_repeat_tokenized, x_val_mul_repeat_normal = clean_text(df_mul_repeat_val['tweets'].values)\n",
        "x_test_mul_repeat_tokenized, x_test_mul_repeat_normal = clean_text(df_mul_repeat_test['tweets'].values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "       ##### Lowercasing Done! Time Taken -  0.010810375213623047\n",
            "\n",
            "       ##### Punctuation removed! Time Taken -  0.1181480884552002\n",
            "\n",
            "       ##### Whitespace removed! Time Taken -  0.03280997276306152\n",
            "\n",
            "       ##### Tokenization Done using NLTK! Time Taken -  2.2395598888397217\n",
            "\n",
            "       ##### Lowercasing Done! Time Taken -  0.0016427040100097656\n",
            "\n",
            "       ##### Punctuation removed! Time Taken -  0.01767277717590332\n",
            "\n",
            "       ##### Whitespace removed! Time Taken -  0.004660844802856445\n",
            "\n",
            "       ##### Tokenization Done using NLTK! Time Taken -  0.321491003036499\n",
            "\n",
            "       ##### Lowercasing Done! Time Taken -  0.0034494400024414062\n",
            "\n",
            "       ##### Punctuation removed! Time Taken -  0.03859567642211914\n",
            "\n",
            "       ##### Whitespace removed! Time Taken -  0.009351968765258789\n",
            "\n",
            "       ##### Tokenization Done using NLTK! Time Taken -  0.6670784950256348\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHe_Lu8A3DdZ"
      },
      "source": [
        "y_mul_repeat_train = df_mul_repeat_train['follows?'].values\n",
        "y_mul_repeat_val = df_mul_repeat_val['follows?'].values\n",
        "y_mul_repeat_test = df_mul_repeat_test['follows?'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmM9dPN4EYgm",
        "outputId": "c8c34004-ef23-4943-d1ce-0ea5d3553c43"
      },
      "source": [
        "df_mul_repeat_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4789, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "KqsKFZqxEfSO",
        "outputId": "77ac71cf-5b3f-4d11-8a63-884c7268c9c0"
      },
      "source": [
        "df_mul_repeat_test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>tweets</th>\n",
              "      <th>emoji_sentence</th>\n",
              "      <th>follows?</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20092</td>\n",
              "      <td>[USER] CONGRATS! This is...SO CUTE...//clenche...</td>\n",
              "      <td>üíéüôåüöÄ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12536</td>\n",
              "      <td>Hair Appt Booked  Just Need To Go Shopping For...</td>\n",
              "      <td>‚úäüèΩ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>394</td>\n",
              "      <td>[USER] Lmao so true 3 for y'all 1 for usHoping...</td>\n",
              "      <td>üíô‚ú®</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8301</td>\n",
              "      <td>[USER] Happy happy birthday!!! . Your day soun...</td>\n",
              "      <td>üéâüéâüéâü§Ø</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1226</td>\n",
              "      <td>My coworker hates going to restaurants with me...</td>\n",
              "      <td>üçÆüíú</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  ... follows?\n",
              "0  20092  ...        0\n",
              "1  12536  ...        0\n",
              "2    394  ...        1\n",
              "3   8301  ...        1\n",
              "4   1226  ...        1\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEkXCvSnEILf",
        "outputId": "26132861-8baf-4ae2-caef-f22e5d908f84"
      },
      "source": [
        "print(len(x_test_mul_repeat_tokenized))\n",
        "print(len(y_mul_repeat_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4789\n",
            "4789\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FBVTkrz3Dg8"
      },
      "source": [
        "#remove empty values for train\n",
        "y_train_mul_repeat_2 = []\n",
        "x_train_mul_repeat_tokenized_2 = []\n",
        "x_train_mul_repeat_normal_2 = []\n",
        "empty_indices_mul_repeat_train = []\n",
        "\n",
        "for i in range(len(x_train_mul_repeat_tokenized)):\n",
        "  if len(x_train_mul_repeat_tokenized[i])==0:\n",
        "    empty_indices_mul_repeat_train.append(i)\n",
        "  else:\n",
        "    x_train_mul_repeat_tokenized_2.append(x_train_mul_repeat_tokenized[i])\n",
        "    x_train_mul_repeat_normal_2.append(x_train_mul_repeat_normal[i])\n",
        "    y_train_mul_repeat_2.append(y_mul_repeat_train[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAHZ1Bw-3Dkh"
      },
      "source": [
        "#remove empty values for val \n",
        "y_val_mul_repeat_2 = []\n",
        "x_val_mul_repeat_tokenized_2 = []\n",
        "x_val_mul_repeat_normal_2 = []\n",
        "empty_indices_mul_repeat_val = []\n",
        "\n",
        "for i in range(len(x_val_mul_repeat_tokenized)):\n",
        "  if len(x_val_mul_repeat_tokenized[i])==0:\n",
        "    empty_indices_mul_repeat_val.append(i)\n",
        "  else:\n",
        "    x_val_mul_repeat_tokenized_2.append(x_val_mul_repeat_tokenized[i])\n",
        "    x_val_mul_repeat_normal_2.append(x_val_mul_repeat_normal[i])\n",
        "    y_val_mul_repeat_2.append(y_mul_repeat_val[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LJl-PaH5hCl"
      },
      "source": [
        "#remove empty values for test\n",
        "y_test_mul_repeat_2 = []\n",
        "x_test_mul_repeat_tokenized_2 = []\n",
        "x_test_mul_repeat_normal_2 = []\n",
        "empty_indices_mul_repeat_test = []\n",
        "\n",
        "for i in range(len(x_test_mul_repeat_tokenized)):\n",
        "  if len(x_test_mul_repeat_tokenized[i])==0:\n",
        "    empty_indices_mul_repeat_test.append(i)\n",
        "  else:\n",
        "    x_test_mul_repeat_tokenized_2.append(x_test_mul_repeat_tokenized[i])\n",
        "    x_test_mul_repeat_normal_2.append(x_test_mul_repeat_normal[i])\n",
        "    y_test_mul_repeat_2.append(y_mul_repeat_test[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyTLTBi_5hGI",
        "outputId": "c6d10b30-3a82-43b0-e700-105ff8902456"
      },
      "source": [
        "print(empty_indices_mul_repeat_train)\n",
        "print(len(empty_indices_mul_repeat_train))\n",
        "print(len(x_train_mul_repeat_tokenized))\n",
        "print(len(x_train_mul_repeat_tokenized_2))\n",
        "print(len(x_train_mul_repeat_normal_2))\n",
        "print(len(y_train_mul_repeat_2))\n",
        "\n",
        "print(empty_indices_mul_repeat_val)\n",
        "print(len(empty_indices_mul_repeat_val))\n",
        "print(len(x_val_mul_repeat_tokenized))\n",
        "print(len(x_val_mul_repeat_tokenized_2))\n",
        "print(len(x_val_mul_repeat_normal_2))\n",
        "print(len(y_val_mul_repeat_2))\n",
        "\n",
        "print(empty_indices_mul_repeat_test)\n",
        "print(len(empty_indices_mul_repeat_test))\n",
        "print(len(x_test_mul_repeat_tokenized))\n",
        "print(len(x_test_mul_repeat_tokenized_2))\n",
        "print(len(x_test_mul_repeat_normal_2))\n",
        "print(len(y_test_mul_repeat_2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "0\n",
            "16801\n",
            "16801\n",
            "16801\n",
            "16801\n",
            "[]\n",
            "0\n",
            "2401\n",
            "2401\n",
            "2401\n",
            "2401\n",
            "[]\n",
            "0\n",
            "4800\n",
            "4800\n",
            "4800\n",
            "4800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEcWNim65hKD"
      },
      "source": [
        "# Saving the objects:\n",
        "with open('/content/drive/MyDrive/2021_NLU/data/multi_emoji/mul_repeat_emojis.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
        "    pickle.dump([x_train_mul_repeat_tokenized, x_train_mul_repeat_normal, x_val_mul_repeat_tokenized, x_val_mul_repeat_normal, x_test_mul_repeat_tokenized, x_test_mul_repeat_normal, empty_indices_mul_repeat_train, x_train_mul_repeat_tokenized_2, \\\n",
        "                 x_train_mul_repeat_normal_2, empty_indices_mul_repeat_val, x_val_mul_repeat_tokenized_2, x_val_mul_repeat_normal_2, y_mul_repeat_train, y_mul_repeat_val, y_train_mul_repeat_2, y_val_mul_repeat_2, y_mul_repeat_test, y_test_mul_repeat_2,\\\n",
        "                 empty_indices_mul_repeat_test, x_test_mul_repeat_normal_2, x_test_mul_repeat_tokenized_2], f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHcNhCY8KySi"
      },
      "source": [
        "with open('/content/drive/MyDrive/2021_NLU/data/multi_emoji/mul_repeat_emojis.pkl', 'rb') as f:  \n",
        "    x_train_mul_repeat_tokenized, x_train_mul_repeat_normal, x_val_mul_repeat_tokenized, x_val_mul_repeat_normal, x_test_mul_repeat_tokenized, x_test_mul_repeat_normal, empty_indices_mul_repeat_train, x_train_mul_repeat_tokenized_2, \\\n",
        "                 x_train_mul_repeat_normal_2, empty_indices_mul_repeat_val, x_val_mul_repeat_tokenized_2, x_val_mul_repeat_normal_2, y_mul_repeat_train, y_mul_repeat_val, y_train_mul_repeat_2, y_val_mul_repeat_2, y_mul_repeat_test, y_test_mul_repeat_2,\\\n",
        "                 empty_indices_mul_repeat_test, x_test_mul_repeat_normal_2, x_test_mul_repeat_tokenized_2 = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMpUokhqKyWf"
      },
      "source": [
        "X_train_mul_repeat_w2vec = convert_word2vec(word2vec, x_train_mul_repeat_tokenized_2, strategy='mean')\n",
        "X_val_mul_repeat_w2vec = convert_word2vec(word2vec, x_val_mul_repeat_tokenized_2, strategy='mean')\n",
        "X_test_mul_repeat_w2vec = convert_word2vec(word2vec, x_test_mul_repeat_tokenized_2, strategy='mean')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZIfBBfhK7Q8"
      },
      "source": [
        "# list(df_train['emoji_sentence'][2])\n",
        "df_mul_repeat_train['emoji_sentence_list'] = df_mul_repeat_train['emoji_sentence'].apply(lambda x: list(x))\n",
        "emoji_corpus_mul_repeat_train = df_mul_repeat_train['emoji_sentence_list'].values\n",
        "\n",
        "df_mul_repeat_val['emoji_sentence_list'] = df_mul_repeat_val['emoji_sentence'].apply(lambda x: list(x))\n",
        "emoji_corpus_mul_repeat_val = df_mul_repeat_val['emoji_sentence_list'].values\n",
        "\n",
        "df_mul_repeat_test['emoji_sentence_list'] = df_mul_repeat_test['emoji_sentence'].apply(lambda x: list(x))\n",
        "emoji_corpus_mul_repeat_test = df_mul_repeat_test['emoji_sentence_list'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpxdaL-YK7U0"
      },
      "source": [
        "X_train_mul_repeat_e2vec = convert_emoji2vec(e2v, emoji_corpus_mul_repeat_train, strategy='mean')\n",
        "X_val_mul_repeat_e2vec = convert_emoji2vec(e2v, emoji_corpus_mul_repeat_val, strategy='mean')\n",
        "X_test_mul_repeat_e2vec = convert_emoji2vec(e2v, emoji_corpus_mul_repeat_test, strategy='mean')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORDwv4g5KybJ",
        "outputId": "a2f2f940-9812-4556-ccaf-76fd5bfe70a5"
      },
      "source": [
        "print(len(X_train_mul_repeat_w2vec))\n",
        "print(len(X_train_mul_repeat_w2vec[4]))\n",
        "\n",
        "print(len(X_val_mul_repeat_w2vec))\n",
        "print(len(X_val_mul_repeat_w2vec[4]))\n",
        "\n",
        "print(len(X_test_mul_repeat_w2vec))\n",
        "print(len(X_test_mul_repeat_w2vec[4]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16801\n",
            "300\n",
            "2401\n",
            "300\n",
            "4800\n",
            "300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MRmFUFmLHBR",
        "outputId": "cbd785ca-06f8-4afa-d3aa-f22d64306fad"
      },
      "source": [
        "print(len(X_train_mul_repeat_e2vec))\n",
        "print(len(X_train_mul_repeat_e2vec[4]))\n",
        "\n",
        "print(len(X_val_mul_repeat_e2vec))\n",
        "print(len(X_val_mul_repeat_e2vec[4]))\n",
        "\n",
        "print(len(X_test_mul_repeat_e2vec))\n",
        "print(len(X_test_mul_repeat_e2vec[4]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16801\n",
            "300\n",
            "2401\n",
            "300\n",
            "4800\n",
            "300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fumuF82McSFz"
      },
      "source": [
        "#### Averaged Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpCaYdLzLHFI"
      },
      "source": [
        "X_train_mul_repeat_vec = (np.array(X_train_mul_repeat_w2vec) + np.array(X_train_mul_repeat_e2vec))/2\n",
        "X_val_mul_repeat_vec = (np.array(X_val_mul_repeat_w2vec) + np.array(X_val_mul_repeat_e2vec))/2\n",
        "X_test_mul_repeat_vec = (np.array(X_test_mul_repeat_w2vec) + np.array(X_test_mul_repeat_e2vec))/2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQp-nCDELNGw"
      },
      "source": [
        "# Saving the objects:\n",
        "with open('/content/drive/MyDrive/2021_NLU/data/multi_emoji/mul_repeat_emojis_vec.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
        "    pickle.dump([X_train_mul_repeat_vec, X_val_mul_repeat_vec, X_test_mul_repeat_vec], f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttXwiN2kLNKC"
      },
      "source": [
        "with open('/content/drive/MyDrive/2021_NLU/data/multi_emoji/mul_repeat_emojis_vec.pkl', 'rb') as f:  \n",
        "    X_train_mul_repeat_vec, X_val_mul_repeat_vec, X_test_mul_repeat_vec = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ty0XEM88cVDv"
      },
      "source": [
        "#### Concatenated Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMnVCthj3Dn2"
      },
      "source": [
        "X_train_mul_repeat_vec_concat = np.concatenate((X_train_mul_repeat_w2vec, X_train_mul_repeat_e2vec), axis=1) \n",
        "X_val_mul_repeat_vec_concat  = np.concatenate((X_val_mul_repeat_w2vec, X_val_mul_repeat_e2vec), axis=1) \n",
        "X_test_mul_repeat_vec_concat  = np.concatenate((X_test_mul_repeat_w2vec, X_test_mul_repeat_e2vec), axis=1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdRVmNWOcXbq"
      },
      "source": [
        "# Saving the objects:\n",
        "with open('/content/drive/MyDrive/2021_NLU/data/multi_emoji/mul_repeat_emojis_vec.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
        "    pickle.dump([X_train_mul_repeat_vec_concat, X_val_mul_repeat_vec_concat, X_test_mul_repeat_vec_concat], f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEQ6Ht02cXgh"
      },
      "source": [
        "with open('/content/drive/MyDrive/2021_NLU/data/multi_emoji/mul_repeat_emojis_vec.pkl', 'rb') as f:  \n",
        "    X_train_mul_repeat_vec_concat, X_val_mul_repeat_vec_concat, X_test_mul_repeat_vec_concat = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ons1B4P1EqR"
      },
      "source": [
        "## FULL DATA (SINGLE AND MULTI) WITH NO REPEATS - Download, Preprocess, Create Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A7rZZzF1KPy"
      },
      "source": [
        "df_mul_train = pd.read_csv('/content/drive/MyDrive/2021_NLU/data/no_repeats/emoji_nsp_dataset_no_repeats_train.csv')\n",
        "df_mul_val = pd.read_csv('/content/drive/MyDrive/2021_NLU/data/no_repeats/emoji_nsp_dataset_no_repeats_valid.csv')\n",
        "df_mul_test = pd.read_csv('/content/drive/MyDrive/2021_NLU/data/no_repeats/emoji_nsp_dataset_no_repeats_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "dGECt3HO1Kfk",
        "outputId": "8e8aada5-9edf-46fb-dce3-26e0d13808e6"
      },
      "source": [
        "df_mul_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>tweets</th>\n",
              "      <th>emoji_sentence</th>\n",
              "      <th>follows?</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>50553</td>\n",
              "      <td>The dababy memes make no sense and that‚Äôs why ...</td>\n",
              "      <td>üò≠</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>74541</td>\n",
              "      <td>a year ago today i would be holding my breath ...</td>\n",
              "      <td>üôè</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>50992</td>\n",
              "      <td>I told my mama about how the music industry is...</td>\n",
              "      <td>üíØ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>95343</td>\n",
              "      <td>[USER] [USER] Thankyou guys</td>\n",
              "      <td>üíÄ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>60555</td>\n",
              "      <td>You want new SUBS? Like Ô∏è Retweet Follow me  R...</td>\n",
              "      <td>üò¨</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  ... follows?\n",
              "0  50553  ...        1\n",
              "1  74541  ...        0\n",
              "2  50992  ...        1\n",
              "3  95343  ...        0\n",
              "4  60555  ...        0\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjgLJHO_1Ki2",
        "outputId": "047e9b24-5bb5-49d2-9299-883a7f2ce991"
      },
      "source": [
        "print(len(df_mul_train))\n",
        "print(len(df_mul_val))\n",
        "print(len(df_mul_test))\n",
        "\n",
        "df_mul_train = df_mul_train.dropna().drop_duplicates()\n",
        "df_mul_val = df_mul_val.dropna().drop_duplicates()\n",
        "df_mul_test = df_mul_test.dropna().drop_duplicates()\n",
        "\n",
        "print(len(df_mul_train))\n",
        "print(len(df_mul_val))\n",
        "print(len(df_mul_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15475\n",
            "2229\n",
            "4433\n",
            "15475\n",
            "2229\n",
            "4433\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATl6Dp2B1Klb",
        "outputId": "4940db8a-9566-419a-bb10-478f10a07cb1"
      },
      "source": [
        "# sample_tokenized, sample_normal = clean_text(sample)\n",
        "x_train_mul_tokenized, x_train_mul_normal = clean_text(df_mul_train['tweets'].values)\n",
        "x_val_mul_tokenized, x_val_mul_normal = clean_text(df_mul_val['tweets'].values)\n",
        "x_test_mul_tokenized, x_test_mul_normal = clean_text(df_mul_test['tweets'].values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "       ##### Lowercasing Done! Time Taken -  0.014504194259643555\n",
            "\n",
            "       ##### Punctuation removed! Time Taken -  0.08419013023376465\n",
            "\n",
            "       ##### Whitespace removed! Time Taken -  0.024480104446411133\n",
            "\n",
            "       ##### Tokenization Done using NLTK! Time Taken -  1.8592925071716309\n",
            "\n",
            "       ##### Lowercasing Done! Time Taken -  0.000911712646484375\n",
            "\n",
            "       ##### Punctuation removed! Time Taken -  0.012573957443237305\n",
            "\n",
            "       ##### Whitespace removed! Time Taken -  0.0032439231872558594\n",
            "\n",
            "       ##### Tokenization Done using NLTK! Time Taken -  0.26091432571411133\n",
            "\n",
            "       ##### Lowercasing Done! Time Taken -  0.0018279552459716797\n",
            "\n",
            "       ##### Punctuation removed! Time Taken -  0.024820804595947266\n",
            "\n",
            "       ##### Whitespace removed! Time Taken -  0.008264541625976562\n",
            "\n",
            "       ##### Tokenization Done using NLTK! Time Taken -  0.5202980041503906\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOxMVEsl3FNe"
      },
      "source": [
        "y_mul_train = df_mul_train['follows?'].values\n",
        "y_mul_val = df_mul_val['follows?'].values\n",
        "y_mul_test = df_mul_test['follows?'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yW89ftqe3FQ9"
      },
      "source": [
        "#remove empty values for train\n",
        "y_train_mul_2 = []\n",
        "x_train_mul_tokenized_2 = []\n",
        "x_train_mul_normal_2 = []\n",
        "empty_indices_mul_train = []\n",
        "\n",
        "for i in range(len(x_train_mul_tokenized)):\n",
        "  if len(x_train_mul_tokenized[i])==0:\n",
        "    empty_indices_mul_train.append(i)\n",
        "  else:\n",
        "    x_train_mul_tokenized_2.append(x_train_mul_tokenized[i])\n",
        "    x_train_mul_normal_2.append(x_train_mul_normal[i])\n",
        "    y_train_mul_2.append(y_mul_train[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYJYi5V_3FT3"
      },
      "source": [
        "#remove empty values for val\n",
        "y_val_mul_2 = []\n",
        "x_val_mul_tokenized_2 = []\n",
        "x_val_mul_normal_2 = []\n",
        "empty_indices_mul_val = []\n",
        "\n",
        "for i in range(len(x_val_mul_tokenized)):\n",
        "  if len(x_val_mul_tokenized[i])==0:\n",
        "    empty_indices_mul_val.append(i)\n",
        "  else:\n",
        "    x_val_mul_tokenized_2.append(x_val_mul_tokenized[i])\n",
        "    x_val_mul_normal_2.append(x_val_mul_normal[i])\n",
        "    y_val_mul_2.append(y_mul_val[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80IhXAiW5i4E"
      },
      "source": [
        "#remove empty values for test\n",
        "y_test_mul_2 = []\n",
        "x_test_mul_tokenized_2 = []\n",
        "x_test_mul_normal_2 = []\n",
        "empty_indices_mul_test = []\n",
        "\n",
        "for i in range(len(x_test_mul_tokenized)):\n",
        "  if len(x_test_mul_tokenized[i])==0:\n",
        "    empty_indices_mul_test.append(i)\n",
        "  else:\n",
        "    x_test_mul_tokenized_2.append(x_test_mul_tokenized[i])\n",
        "    x_test_mul_normal_2.append(x_test_mul_normal[i])\n",
        "    y_test_mul_2.append(y_mul_test[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVO6tw575i7p",
        "outputId": "ae78add1-50bb-41ff-a027-83ba22c17582"
      },
      "source": [
        "print(empty_indices_mul_train)\n",
        "print(len(empty_indices_mul_train))\n",
        "print(len(x_train_mul_tokenized))\n",
        "print(len(x_train_mul_tokenized_2))\n",
        "print(len(x_train_mul_normal_2))\n",
        "print(len(y_train_mul_2))\n",
        "\n",
        "print(empty_indices_mul_val)\n",
        "print(len(empty_indices_mul_val))\n",
        "print(len(x_val_mul_tokenized))\n",
        "print(len(x_val_mul_tokenized_2))\n",
        "print(len(x_val_mul_normal_2))\n",
        "print(len(y_val_mul_2))\n",
        "\n",
        "print(empty_indices_mul_test)\n",
        "print(len(empty_indices_mul_test))\n",
        "print(len(x_test_mul_tokenized))\n",
        "print(len(x_test_mul_tokenized_2))\n",
        "print(len(x_test_mul_normal_2))\n",
        "print(len(y_test_mul_2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "0\n",
            "15475\n",
            "15475\n",
            "15475\n",
            "15475\n",
            "[]\n",
            "0\n",
            "2229\n",
            "2229\n",
            "2229\n",
            "2229\n",
            "[]\n",
            "0\n",
            "4433\n",
            "4433\n",
            "4433\n",
            "4433\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GZ2k7npO5Bx"
      },
      "source": [
        "# Saving the objects:\n",
        "with open('/content/drive/MyDrive/2021_NLU/data/no_repeats/mul_emojis.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
        "    pickle.dump([x_train_mul_tokenized, x_train_mul_normal, x_val_mul_tokenized, x_val_mul_normal, x_test_mul_tokenized, x_test_mul_normal, empty_indices_mul_train, x_train_mul_tokenized_2, \\\n",
        "                 x_train_mul_normal_2, empty_indices_mul_val, x_val_mul_tokenized_2, x_val_mul_normal_2, y_mul_train, y_mul_val, y_train_mul_2, y_val_mul_2, y_mul_test, y_test_mul_2,\\\n",
        "                 empty_indices_mul_test, x_test_mul_normal_2, x_test_mul_tokenized_2], f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlP3XYPDO5GG"
      },
      "source": [
        "with open('/content/drive/MyDrive/2021_NLU/data/no_repeats/mul_emojis.pkl', 'rb') as f:  \n",
        "    x_train_mul_tokenized, x_train_mul_normal, x_val_mul_tokenized, x_val_mul_normal, x_test_mul_tokenized, x_test_mul_normal, empty_indices_mul_train, x_train_mul_tokenized_2, \\\n",
        "                 x_train_mul_normal_2, empty_indices_mul_val, x_val_mul_tokenized_2, x_val_mul_normal_2, y_mul_train, y_mul_val, y_train_mul_2, y_val_mul_2, y_mul_test, y_test_mul_2,\\\n",
        "                 empty_indices_mul_test, x_test_mul_normal_2, x_test_mul_tokenized_2 = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeUzJvzjO5KN"
      },
      "source": [
        "X_train_mul_w2vec = convert_word2vec(word2vec, x_train_mul_tokenized_2, strategy='mean')\n",
        "X_val_mul_w2vec = convert_word2vec(word2vec, x_val_mul_tokenized_2, strategy='mean')\n",
        "X_test_mul_w2vec = convert_word2vec(word2vec, x_test_mul_tokenized_2, strategy='mean')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTeLkMWZO5RB"
      },
      "source": [
        "# list(df_train['emoji_sentence'][2])\n",
        "df_mul_train['emoji_sentence_list'] = df_mul_train['emoji_sentence'].apply(lambda x: list(x))\n",
        "emoji_corpus_mul_train = df_mul_train['emoji_sentence_list'].values\n",
        "\n",
        "df_mul_val['emoji_sentence_list'] = df_mul_val['emoji_sentence'].apply(lambda x: list(x))\n",
        "emoji_corpus_mul_val = df_mul_val['emoji_sentence_list'].values\n",
        "\n",
        "df_mul_test['emoji_sentence_list'] = df_mul_test['emoji_sentence'].apply(lambda x: list(x))\n",
        "emoji_corpus_mul_test = df_mul_test['emoji_sentence_list'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaH825a4O5Ve"
      },
      "source": [
        "X_train_mul_e2vec = convert_emoji2vec(e2v, emoji_corpus_mul_train, strategy='mean')\n",
        "X_val_mul_e2vec = convert_emoji2vec(e2v, emoji_corpus_mul_val, strategy='mean')\n",
        "X_test_mul_e2vec = convert_emoji2vec(e2v, emoji_corpus_mul_test, strategy='mean')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hx-bLILP5jBg",
        "outputId": "bc8d7b10-16be-4ca0-fef4-b80c853ff22a"
      },
      "source": [
        "print(len(X_train_mul_w2vec))\n",
        "print(len(X_train_mul_w2vec[4]))\n",
        "\n",
        "print(len(X_val_mul_w2vec))\n",
        "print(len(X_val_mul_w2vec[4]))\n",
        "\n",
        "print(len(X_test_mul_w2vec))\n",
        "print(len(X_test_mul_w2vec[4]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15475\n",
            "300\n",
            "2229\n",
            "300\n",
            "4433\n",
            "300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9hS1zRIPK6D",
        "outputId": "b84245bf-c6ea-4ab9-fde4-977d0949779d"
      },
      "source": [
        "print(len(X_train_mul_e2vec))\n",
        "print(len(X_train_mul_e2vec[4]))\n",
        "\n",
        "print(len(X_val_mul_e2vec))\n",
        "print(len(X_val_mul_e2vec[4]))\n",
        "\n",
        "print(len(X_test_mul_e2vec))\n",
        "print(len(X_test_mul_e2vec[4]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15475\n",
            "300\n",
            "2229\n",
            "300\n",
            "4433\n",
            "300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Pe5JYUocxfh"
      },
      "source": [
        "#### Averaged Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVRlEsh5PK-e"
      },
      "source": [
        "X_train_mul_vec = (np.array(X_train_mul_w2vec) + np.array(X_train_mul_e2vec))/2\n",
        "X_val_mul_vec = (np.array(X_val_mul_w2vec) + np.array(X_val_mul_e2vec))/2\n",
        "X_test_mul_vec = (np.array(X_test_mul_w2vec) + np.array(X_test_mul_e2vec))/2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf9jZyZEPRMD"
      },
      "source": [
        "# Saving the objects:\n",
        "with open('/content/drive/MyDrive/2021_NLU/data/no_repeats/mul_emojis_vec.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
        "    pickle.dump([X_train_mul_vec, X_val_mul_vec, X_test_mul_vec], f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unYdKmdvPRQ9"
      },
      "source": [
        "with open('/content/drive/MyDrive/2021_NLU/data/no_repeats/mul_emojis_vec.pkl', 'rb') as f:  \n",
        "    X_train_mul_vec, X_val_mul_vec, X_test_mul_vec = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgqpUpnFc1ET"
      },
      "source": [
        "#### Concatenated Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qHS0wqA1Koi"
      },
      "source": [
        "X_train_mul_vec_concat = np.concatenate((X_train_mul_w2vec, X_train_mul_e2vec), axis=1) \n",
        "X_val_mul_vec_concat  = np.concatenate((X_val_mul_w2vec, X_val_mul_e2vec), axis=1) \n",
        "X_test_mul_vec_concat  = np.concatenate((X_test_mul_w2vec, X_test_mul_e2vec), axis=1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XN_AZ-Ac4Pb"
      },
      "source": [
        "# Saving the objects:\n",
        "with open('/content/drive/MyDrive/2021_NLU/data/no_repeats/mul_emojis_vec.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
        "    pickle.dump([X_train_mul_vec_concat, X_val_mul_vec_concat, X_test_mul_vec_concat], f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8YK-WvQc4VI"
      },
      "source": [
        "with open('/content/drive/MyDrive/2021_NLU/data/no_repeats/mul_emojis_vec.pkl', 'rb') as f:  \n",
        "    X_train_mul_vec_concat, X_val_mul_vec_concat, X_test_mul_vec_concat = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}