# -*- coding: utf-8 -*-
"""NLU_emoji_project_LR_Baseline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IyQkQJB2btdW22cEVugvBlS7GPllzn46
"""

from google.colab import drive
drive.mount('/content/drive')

import gensim.models as gsm
import gensim.downloader
import pandas as pd
import time
import string
import nltk
nltk.download('punkt')
import numpy as np
import pickle
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from gensim.models import Word2Vec
import matplotlib.pyplot as plt

"""**Emoji2Vec Download**"""

e2v = gsm.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/2021_NLU/emoji2vec.bin', binary=True)

e2v.vector_size

happy_vector = e2v['ðŸ˜‚']  
happy_vector.shape

print(len(e2v.vocab))
print(e2v.vocab.keys())

"""**Word2Vec Download**"""

word2vec = gensim.downloader.load('word2vec-google-news-300')

pickle.dump(word2vec, open('/content/drive/MyDrive/2021_NLU/data/full_data/word2vec.pkl', 'wb'))

with open('/content/drive/MyDrive/2021_NLU/data/full_data/word2vec.pkl', 'rb') as f:  
    word2vec = pickle.load(f)

"""## FULL DATA - Download, Preprocess, Create Vectors"""

df_train = pd.read_csv('/content/drive/MyDrive/2021_NLU/data/full_data/emoji_nsp_dataset_train.csv')

df_val = pd.read_csv('/content/drive/MyDrive/2021_NLU/data/full_data/emoji_nsp_dataset_valid.csv')

df_test = pd.read_csv('/content/drive/MyDrive/2021_NLU/data/full_data/emoji_nsp_dataset_test.csv')

df_train.head()

df_train.count()

df_train["emoji_sentence"]

df_train["emoji_sentence"].isna()

print(len(df_train))
print(len(df_val))
print(len(df_test))

df_train = df_train.dropna().drop_duplicates()
df_val = df_val.dropna().drop_duplicates()
df_test = df_test.dropna().drop_duplicates()

print(len(df_train))
print(len(df_val))
print(len(df_test))

"""###Preprocessing"""

def make_lowercase(data, debug=False):
	'''
	- input: data - list of documents
	- output: data - list of documents after lowercasing everything
	'''
	if(debug):
		print("data_sample out of ",len(data))
		print(data[:sample_to_print])
	start = time.time()
	data = [i.lower() for i in data]

	end = time.time()
	print('\n       ##### Lowercasing Done! Time Taken - ',end-start)
	return data                                                                       


def punctuation_removal(data, debug=False):
	'''
	- input: data - list of documents
	- output: data - list of documents after removing punctuation
	'''
	if(debug):
		print("data_sample out of ",len(data))
		print(data[:sample_to_print])
	start = time.time()
	data = [i.translate(str.maketrans(string.punctuation,' '*len(string.punctuation))) for i in data]
	end = time.time()
	print('\n       ##### Punctuation removed! Time Taken - ',end-start)
	return data

def whitespace_removal(data, debug=False):
	'''
	- input: data - 
	- output: data - 
	'''
	if(debug):
		print("data_sample out of ",len(data))
		print(data[:sample_to_print])
	start = time.time()
	data = [' '.join(mystring.split()) for mystring in data]
	# data = [i.strip() for i in data]
	end = time.time()
	print('\n       ##### Whitespace removed! Time Taken - ',end-start)
	return data

# TOKENIZATION with NLTK
def tokenization_nltk(data, debug=False):
	'''
	- input: data - 
	- output: data - 
	'''
	if(debug):
		print("data_sample out of ",len(data))
		print(data[:sample_to_print])
	# Using NLTK
	start = time.time()
	data = [nltk.word_tokenize(i) for i in data]
	end = time.time()
	# Using Spacy - Spacy takes too much time
	#data = [[token.text for token in nlp_spacy(i)] for i in data]
	print('\n       ##### Tokenization Done using NLTK! Time Taken - ', end-start)
	return data

# #used to search in nltk stop_words
# def BinarySearch(a, x): 
# 	i = bisect_left(a, x) 
# 	if i != len(a) and a[i] == x:
# 		return i 
# 	else: 
# 		return -1

# def stopwords_removal(data, stop_words_nltk, debug=False):
# 	'''
# 	- input: data - 
# 	- output: data - 
# 	'''
# 	if(debug):
# 		print("stopwords_removal_nltk data_sample out of ",len(data))
# 		print(data[:sample_to_print])
# 	#using NLTK
# 	start = time.time()
# 	data = [[j for j in doc if (BinarySearch(stop_words_nltk,j)<0)] for doc in data]
# 	data = [[x for x in word if not (x.isdigit() or x[0] == '-' and x[1:].isdigit())] for word in data]
# 	end = time.time()
# 	print('\n       ##### Stopwords Removed using NLTK! Time Taken - ',end-start)
# 	return data

def clean_text(sample, debug=False):
  '''
  sample should be a list of documents
  '''


  # sample = remove_string_with_nonASCII(sample)
  # if debug:
  #   print(sample[:2])

  # sample = preprocess_tweet_text(sample)
  # if debug:
  #   print(sample[:2])
    
  sample = make_lowercase(sample)
  if debug:
    print(sample[:2])

  sample = punctuation_removal(sample)
  if debug:
    print(sample[:2])

  sample = whitespace_removal(sample)
  if debug:
    print(sample[:2])

  sample = tokenization_nltk(sample)
  if debug:
    print(sample[:2])

  # sample = tokenization_spacy(sample)
  # if debug:
  #   print(sample[:2])

  # sample = lemmatization_tokenization_spacy(sample)
  # if debug:
    # print(sample[:2])

  # sample = stopwords_removal(sample, stop_words_nltk)
  # if debug:
  #   print(sample[:2])

  # sample = make_bigrams_gensim(sample, bigrams_min_count=10, bigrams_threshold=10) #params from gensim
  # if debug:
  #   print(sample[:2])

  sample_normal = [' '.join(i) for i in sample]
  # Sample tokenized is used for Word2Vec

  return sample, sample_normal

# sample_tokenized, sample_normal = clean_text(sample)
x_train_tokenized, x_train_normal = clean_text(df_train['tweets'].values)
x_val_tokenized, x_val_normal = clean_text(df_val['tweets'].values)
x_test_tokenized, x_test_normal = clean_text(df_test['tweets'].values)

y_train = df_train['follows?'].values
y_val = df_val['follows?'].values
y_test = df_test['follows?'].values

#remove empty values for train
y_train_2 = []
x_train_tokenized_2 = []
x_train_normal_2 = []
empty_indices_train = []

for i in range(len(x_train_tokenized)):
  if len(x_train_tokenized[i])==0:
    empty_indices_train.append(i)
  else:
    x_train_tokenized_2.append(x_train_tokenized[i])
    x_train_normal_2.append(x_train_normal[i])
    y_train_2.append(y_train[i])

#remove empty values for val
y_val_2 = []
x_val_tokenized_2 = []
x_val_normal_2 = []
empty_indices_val = []

for i in range(len(x_val_tokenized)):
  if len(x_val_tokenized[i])==0:
    empty_indices_val.append(i)
  else:
    x_val_tokenized_2.append(x_val_tokenized[i])
    x_val_normal_2.append(x_val_normal[i])
    y_val_2.append(y_val[i])

#remove empty values for test
y_test_2 = []
x_test_tokenized_2 = []
x_test_normal_2 = []
empty_indices_test = []

for i in range(len(x_test_tokenized)):
  if len(x_test_tokenized[i])==0:
    empty_indices_test.append(i)
  else:
    x_test_tokenized_2.append(x_test_tokenized[i])
    x_test_normal_2.append(x_test_normal[i])
    y_test_2.append(y_test[i])

print(empty_indices_train)
print(len(empty_indices_train))
print(len(x_train_tokenized))
print(len(x_train_tokenized_2))
print(len(x_train_normal_2))
print(len(y_train_2))

print(empty_indices_val)
print(len(empty_indices_val))
print(len(x_val_tokenized))
print(len(x_val_tokenized_2))
print(len(x_val_normal_2))
print(len(y_val_2))

print(empty_indices_test)
print(len(empty_indices_test))
print(len(x_test_tokenized))
print(len(x_test_tokenized_2))
print(len(x_test_normal_2))
print(len(y_test_2))

"""### Save tokens"""

# Saving the objects:
with open('/content/drive/MyDrive/2021_NLU/data/full_data/full_data.pkl', 'wb') as f:  # Python 3: open(..., 'wb')
    pickle.dump([x_train_tokenized, x_train_normal, x_val_tokenized, x_val_normal, x_test_tokenized, x_test_normal, empty_indices_train, x_train_tokenized_2, \
                 x_train_normal_2, empty_indices_val, x_val_tokenized_2, x_val_normal_2, y_train, y_val, y_train_2, y_val_2, y_test, y_test_2,\
                 empty_indices_test, x_test_normal_2, x_test_tokenized_2 ], f)

with open('/content/drive/MyDrive/2021_NLU/data/full_data/full_data.pkl', 'rb') as f:  
    x_train_tokenized, x_train_normal, x_val_tokenized, x_val_normal, x_test_tokenized, x_test_normal, empty_indices_train, x_train_tokenized_2, \
                 x_train_normal_2, empty_indices_val, x_val_tokenized_2, x_val_normal_2, y_train, y_val, y_train_2, y_val_2, y_test, y_test_2,\
                 empty_indices_test, x_test_normal_2, x_test_tokenized_2 = pickle.load(f)

# def create_df(xdata, emojidata, ydata):
#   temp=[" ".join(i) for i in xdata]
#   df_new = pd.DataFrame(temp)
#   df_new["Emoji"] = emojidata
#   df_new["Target"] = ydata

#   df_new.columns = ["Tweet", "Emoji", "Target"]
#   return df_new

# df_new_val = create_df(x_val_tokenized_2,  y_val_2)
# df_new_train = create_df(x_train_tokenized_2, y_train_2)
# df_new_test = create_df(x_test_tokenized_2, df_test['emoji_sentence'], y_test_2)

# df_new_test.head()

df_test['tokenized_tweets'] = x_test_tokenized
df_test['tokenized_len'] = df_test['tokenized_tweets'].apply(lambda x: len(x))
print(df_test['tokenized_len'].mean())
print(df_test['tokenized_len'].median())

# # Saving the objects:
# with open('/content/drive/MyDrive/2021_NLU/data/full_data/df_new.pkl', 'wb') as f:  # Python 3: open(..., 'wb')
#     pickle.dump([df_new_val, df_new_train, df_new_test], f)

# with open('/content/drive/MyDrive/2020 NLP/Project/df_new.pkl', 'rb') as f:  
#     df_new_val, df_new_train, df_new_test = pickle.load(f)

"""### Create Vectors"""

def convert_word2vec(model, corpus, strategy):
  # return [[model[token] for token in sentence] for sentence in corpus]
  output = []
  for sentence in corpus:
    vector_ = np.zeros(model.vector_size)
    for token in sentence:
      try:
        token_vector = model[token]
        vector_ = vector_ + token_vector
      except:
        vector_ = vector_ + np.zeros(model.vector_size)
    if strategy=='mean':
      vector_ = vector_/len(sentence)
    elif strategy=='add':
      pass
    output.append(vector_)
  # output is a list
  return output

X_train_w2vec = convert_word2vec(word2vec, x_train_tokenized_2, strategy='mean')
X_val_w2vec = convert_word2vec(word2vec, x_val_tokenized_2, strategy='mean')
X_test_w2vec = convert_word2vec(word2vec, x_test_tokenized_2, strategy='mean')

# list(df_train['emoji_sentence'][2])
df_train['emoji_sentence_list'] = df_train['emoji_sentence'].apply(lambda x: list(x))
emoji_corpus_train = df_train['emoji_sentence_list'].values

df_val['emoji_sentence_list'] = df_val['emoji_sentence'].apply(lambda x: list(x))
emoji_corpus_val = df_val['emoji_sentence_list'].values

df_test['emoji_sentence_list'] = df_test['emoji_sentence'].apply(lambda x: list(x))
emoji_corpus_test = df_test['emoji_sentence_list'].values

def convert_emoji2vec(model, corpus, strategy):
  # return [[model[token] for token in sentence] for sentence in corpus]
  output = []
  for emojis in corpus:
    vector_ = np.zeros(model.vector_size)
    for emoji in emojis:
      try:
        token_vector = model[emoji]
        vector_ = vector_ + token_vector
      except:
        vector_ = vector_ + np.zeros(model.vector_size)
    if strategy=='mean':
      vector_ = vector_/len(emojis)
    elif strategy=='add':
      pass
    output.append(vector_)
  # output is a list
  return output

X_train_e2vec = convert_emoji2vec(e2v, emoji_corpus_train, strategy='mean')
X_val_e2vec = convert_emoji2vec(e2v, emoji_corpus_val, strategy='mean')
X_test_e2vec = convert_emoji2vec(e2v, emoji_corpus_test, strategy='mean')

print(len(X_train_w2vec))
print(len(X_train_w2vec[4]))

print(len(X_val_w2vec))
print(len(X_val_w2vec[4]))

print(len(X_test_w2vec))
print(len(X_test_w2vec[4]))

print(len(X_train_e2vec))
print(len(X_train_e2vec[4]))

print(len(X_val_e2vec))
print(len(X_val_e2vec[4]))

print(len(X_test_e2vec))
print(len(X_test_e2vec[4]))

"""#### Averaged Vectors """

X_train_vec = (np.array(X_train_w2vec) + np.array(X_train_e2vec))/2
X_val_vec = (np.array(X_val_w2vec) + np.array(X_val_e2vec))/2
X_test_vec = (np.array(X_test_w2vec) + np.array(X_test_e2vec))/2

X_train_w2vec[0][:4]

X_train_e2vec[0][:4]

X_train_vec[0][:4]

len(X_train_vec)

"""#### Concantenated Vectors"""

X_train_vec_concat = np.concatenate((X_train_w2vec, X_train_e2vec), axis=1) 
X_val_vec_concat  = np.concatenate((X_val_w2vec, X_val_e2vec), axis=1) 
X_test_vec_concat  = np.concatenate((X_test_w2vec, X_test_e2vec), axis=1)

print(len(X_train_w2vec))
print(len(X_train_e2vec))
print(len(X_train_vec_concat))
print(len(X_train_w2vec[4]))
print(len(X_train_vec_concat[4]))

len(X_train_vec_concat)

X_train_vec_concat.shape

"""### Save Vectors"""

# Saving the objects:
with open('/content/drive/MyDrive/2021_NLU/data/full_data/full_vec.pkl', 'wb') as f:  # Python 3: open(..., 'wb')
    pickle.dump([X_train_vec, X_val_vec, X_test_vec], f)

with open('/content/drive/MyDrive/2021_NLU/data/full_data/full_vec.pkl', 'rb') as f:  
    X_train_vec, X_val_vec, X_test_vec = pickle.load(f)

# Saving the objects:
with open('/content/drive/MyDrive/2021_NLU/data/full_data/full_vec_concat.pkl', 'wb') as f:  # Python 3: open(..., 'wb')
    pickle.dump([X_train_vec_concat, X_val_vec_concat, X_test_vec_concat], f)

with open('/content/drive/MyDrive/2021_NLU/data/full_data/full_vec.pkl', 'rb') as f:  
    X_train_vec_concat, X_val_vec_concat, X_test_vec_concat = pickle.load(f)

"""## SINGLE EMOJI - Download, Preprocess, Create Vectors"""

df_sing_train = pd.read_csv('/content/drive/MyDrive/2021_NLU/data/single_emoji/emoji_nsp_dataset_single_emoji_train.csv')
df_sing_val = pd.read_csv('/content/drive/MyDrive/2021_NLU/data/single_emoji/emoji_nsp_dataset_single_emoji_valid.csv')
df_sing_test = pd.read_csv('/content/drive/MyDrive/2021_NLU/data/single_emoji/emoji_nsp_dataset_single_emoji_test.csv')

df_sing_train.head()

print(len(df_sing_train))
print(len(df_sing_val))
print(len(df_sing_test))

df_sing_train = df_sing_train.dropna().drop_duplicates()
df_sing_val = df_sing_val.dropna().drop_duplicates()
df_sing_test = df_sing_test.dropna().drop_duplicates()

print(len(df_sing_train))
print(len(df_sing_val))
print(len(df_sing_test))

# sample_tokenized, sample_normal = clean_text(sample)
x_train_sing_tokenized, x_train_sing_normal = clean_text(df_sing_train['tweets'].values)
x_val_sing_tokenized, x_val_sing_normal = clean_text(df_sing_val['tweets'].values)
x_test_sing_tokenized, x_test_sing_normal = clean_text(df_sing_test['tweets'].values)

y_sing_train = df_sing_train['follows?'].values
y_sing_val = df_sing_val['follows?'].values
y_sing_test = df_sing_test['follows?'].values

#remove empty values for train
y_train_sing_2 = []
x_train_sing_tokenized_2 = []
x_train_sing_normal_2 = []
empty_indices_sing_train = []

for i in range(len(x_train_sing_tokenized)):
  if len(x_train_sing_tokenized[i])==0:
    empty_indices_sing_train.append(i)
  else:
    x_train_sing_tokenized_2.append(x_train_sing_tokenized[i])
    x_train_sing_normal_2.append(x_train_sing_normal[i])
    y_train_sing_2.append(y_sing_train[i])

#remove empty values for val
y_val_sing_2 = []
x_val_sing_tokenized_2 = []
x_val_sing_normal_2 = []
empty_indices_sing_val = []

for i in range(len(x_val_sing_tokenized)):
  if len(x_val_sing_tokenized[i])==0:
    empty_indices_sing_val.append(i)
  else:
    x_val_sing_tokenized_2.append(x_val_sing_tokenized[i])
    x_val_sing_normal_2.append(x_val_sing_normal[i])
    y_val_sing_2.append(y_sing_val[i])

#remove empty values for test
y_test_sing_2 = []
x_test_sing_tokenized_2 = []
x_test_sing_normal_2 = []
empty_indices_sing_test = []

for i in range(len(x_test_sing_tokenized)):
  if len(x_test_sing_tokenized[i])==0:
    empty_indices_sing_test.append(i)
  else:
    x_test_sing_tokenized_2.append(x_test_sing_tokenized[i])
    x_test_sing_normal_2.append(x_test_sing_normal[i])
    y_test_sing_2.append(y_sing_test[i])

print(empty_indices_sing_train)
print(len(empty_indices_sing_train))
print(len(x_train_sing_tokenized))
print(len(x_train_sing_tokenized_2))
print(len(x_train_sing_normal_2))
print(len(y_train_sing_2))

print(empty_indices_sing_val)
print(len(empty_indices_sing_val))
print(len(x_val_sing_tokenized))
print(len(x_val_sing_tokenized_2))
print(len(x_val_sing_normal_2))
print(len(y_val_sing_2))

print(empty_indices_sing_test)
print(len(empty_indices_sing_test))
print(len(x_test_sing_tokenized))
print(len(x_test_sing_tokenized_2))
print(len(x_test_sing_normal_2))
print(len(y_test_sing_2))

# Saving the objects:
with open('/content/drive/MyDrive/2021_NLU/data/single_emoji/single_emoji.pkl', 'wb') as f:  # Python 3: open(..., 'wb')
    pickle.dump([x_train_sing_tokenized, x_train_sing_normal, x_val_sing_tokenized, x_val_sing_normal, x_test_sing_tokenized, x_test_sing_normal, empty_indices_sing_train, x_train_sing_tokenized_2, \
                 x_train_sing_normal_2, empty_indices_sing_val, x_val_sing_tokenized_2, x_val_sing_normal_2, y_sing_train, y_sing_val, y_train_sing_2, y_val_sing_2, y_sing_test, y_test_sing_2,\
                 empty_indices_sing_test, x_test_sing_normal_2, x_test_sing_tokenized_2 ], f)

with open('/content/drive/MyDrive/2021_NLU/data/single_emoji/single_emoji.pkl', 'rb') as f:  
    x_train_sing_tokenized, x_train_sing_normal, x_val_sing_tokenized, x_val_sing_normal, x_test_sing_tokenized, x_test_sing_normal, empty_indices_sing_train, x_train_sing_tokenized_2, \
                 x_train_sing_normal_2, empty_indices_sing_val, x_val_sing_tokenized_2, x_val_sing_normal_2, y_sing_train, y_sing_val, y_train_sing_2, y_val_sing_2, y_sing_test, y_test_sing_2,\
                 empty_indices_sing_test, x_test_sing_normal_2, x_test_sing_tokenized_2 = pickle.load(f)

len(x_train_sing_tokenized_2)

X_train_sing_w2vec = convert_word2vec(word2vec, x_train_sing_tokenized_2, strategy='mean')
X_val_sing_w2vec = convert_word2vec(word2vec, x_val_sing_tokenized_2, strategy='mean')
X_test_sing_w2vec = convert_word2vec(word2vec, x_test_sing_tokenized_2, strategy='mean')

# list(df_train['emoji_sentence'][2])
df_sing_train['emoji_sentence_list'] = df_sing_train['emoji_sentence'].apply(lambda x: list(x))
emoji_corpus_sing_train = df_sing_train['emoji_sentence_list'].values

df_sing_val['emoji_sentence_list'] = df_sing_val['emoji_sentence'].apply(lambda x: list(x))
emoji_corpus_sing_val = df_sing_val['emoji_sentence_list'].values

df_sing_test['emoji_sentence_list'] = df_sing_test['emoji_sentence'].apply(lambda x: list(x))
emoji_corpus_sing_test = df_sing_test['emoji_sentence_list'].values

X_train_sing_e2vec = convert_emoji2vec(e2v, emoji_corpus_sing_train, strategy='mean')
X_val_sing_e2vec = convert_emoji2vec(e2v, emoji_corpus_sing_val, strategy='mean')
X_test_sing_e2vec = convert_emoji2vec(e2v, emoji_corpus_sing_test, strategy='mean')

print(len(X_train_sing_w2vec))
print(len(X_train_sing_w2vec[4]))

print(len(X_val_sing_w2vec))
print(len(X_val_sing_w2vec[4]))

print(len(X_test_sing_w2vec))
print(len(X_test_sing_w2vec[4]))

print(len(X_train_sing_e2vec))
print(len(X_train_sing_e2vec[4]))

print(len(X_val_sing_e2vec))
print(len(X_val_sing_e2vec[4]))

print(len(X_test_sing_e2vec))
print(len(X_test_sing_e2vec[4]))

"""#### Averaged Vectors"""

X_train_sing_vec = (np.array(X_train_sing_w2vec) + np.array(X_train_sing_e2vec))/2
X_val_sing_vec = (np.array(X_val_sing_w2vec) + np.array(X_val_sing_e2vec))/2
X_test_sing_vec = (np.array(X_test_sing_w2vec) + np.array(X_test_sing_e2vec))/2

# Saving the objects:
with open('/content/drive/MyDrive/2021_NLU/data/single_emoji/single_emoji_vec.pkl', 'wb') as f:  # Python 3: open(..., 'wb')
    pickle.dump([X_train_sing_vec, X_val_sing_vec, X_test_sing_vec], f)

with open('/content/drive/MyDrive/2021_NLU/data/single_emoji/single_emoji_vec.pkl', 'rb') as f:  
    X_train_sing_vec, X_val_sing_vec, X_test_sing_vec = pickle.load(f)

"""#### Concatenated Vectors"""

X_train_sing_vec_concat = np.concatenate((X_train_sing_w2vec, X_train_sing_e2vec), axis=1) 
X_val_sing_vec_concat  = np.concatenate((X_val_sing_w2vec, X_val_sing_e2vec), axis=1) 
X_test_sing_vec_concat  = np.concatenate((X_test_sing_w2vec, X_test_sing_e2vec), axis=1)

# Saving the objects:
with open('/content/drive/MyDrive/2021_NLU/data/single_emoji/single_emoji_vec_concat.pkl', 'wb') as f:  # Python 3: open(..., 'wb')
    pickle.dump([X_train_sing_vec_concat, X_val_sing_vec_concat, X_test_sing_vec_concat], f)

with open('/content/drive/MyDrive/2021_NLU/data/single_emoji/single_emoji_vec_concat.pkl', 'rb') as f:  
    X_train_sing_vec_concat, X_val_sing_vec_concat, X_test_sing_vec_concat = pickle.load(f)

"""## MULTI EMOJIS WITH REPEATS - Download, Preprocess, Create Vectors"""

df_mul_repeat_train = pd.read_csv('/content/drive/MyDrive/2021_NLU/data/multi_emoji/emoji_nsp_dataset_multi_emoji_train.csv')
df_mul_repeat_val = pd.read_csv('/content/drive/MyDrive/2021_NLU/data/multi_emoji/emoji_nsp_dataset_multi_emoji_valid.csv')
df_mul_repeat_test = pd.read_csv('/content/drive/MyDrive/2021_NLU/data/multi_emoji/emoji_nsp_dataset_multi_emoji_test.csv')

df_mul_repeat_train.head()

print(len(df_mul_repeat_train))
print(len(df_mul_repeat_val))
print(len(df_mul_repeat_test))

df_mul_repeat_train = df_mul_repeat_train.dropna().drop_duplicates()
df_mul_repeat_val = df_mul_repeat_val.dropna().drop_duplicates()
df_mul_repeat_test = df_mul_repeat_test.dropna().drop_duplicates()

print(len(df_mul_repeat_train))
print(len(df_mul_repeat_val))
print(len(df_mul_repeat_test))

# sample_tokenized, sample_normal = clean_text(sample)
x_train_mul_repeat_tokenized, x_train_mul_repeat_normal = clean_text(df_mul_repeat_train['tweets'].values)
x_val_mul_repeat_tokenized, x_val_mul_repeat_normal = clean_text(df_mul_repeat_val['tweets'].values)
x_test_mul_repeat_tokenized, x_test_mul_repeat_normal = clean_text(df_mul_repeat_test['tweets'].values)

y_mul_repeat_train = df_mul_repeat_train['follows?'].values
y_mul_repeat_val = df_mul_repeat_val['follows?'].values
y_mul_repeat_test = df_mul_repeat_test['follows?'].values

df_mul_repeat_test.shape

df_mul_repeat_test.head()

print(len(x_test_mul_repeat_tokenized))
print(len(y_mul_repeat_test))

#remove empty values for train
y_train_mul_repeat_2 = []
x_train_mul_repeat_tokenized_2 = []
x_train_mul_repeat_normal_2 = []
empty_indices_mul_repeat_train = []

for i in range(len(x_train_mul_repeat_tokenized)):
  if len(x_train_mul_repeat_tokenized[i])==0:
    empty_indices_mul_repeat_train.append(i)
  else:
    x_train_mul_repeat_tokenized_2.append(x_train_mul_repeat_tokenized[i])
    x_train_mul_repeat_normal_2.append(x_train_mul_repeat_normal[i])
    y_train_mul_repeat_2.append(y_mul_repeat_train[i])

#remove empty values for val 
y_val_mul_repeat_2 = []
x_val_mul_repeat_tokenized_2 = []
x_val_mul_repeat_normal_2 = []
empty_indices_mul_repeat_val = []

for i in range(len(x_val_mul_repeat_tokenized)):
  if len(x_val_mul_repeat_tokenized[i])==0:
    empty_indices_mul_repeat_val.append(i)
  else:
    x_val_mul_repeat_tokenized_2.append(x_val_mul_repeat_tokenized[i])
    x_val_mul_repeat_normal_2.append(x_val_mul_repeat_normal[i])
    y_val_mul_repeat_2.append(y_mul_repeat_val[i])

#remove empty values for test
y_test_mul_repeat_2 = []
x_test_mul_repeat_tokenized_2 = []
x_test_mul_repeat_normal_2 = []
empty_indices_mul_repeat_test = []

for i in range(len(x_test_mul_repeat_tokenized)):
  if len(x_test_mul_repeat_tokenized[i])==0:
    empty_indices_mul_repeat_test.append(i)
  else:
    x_test_mul_repeat_tokenized_2.append(x_test_mul_repeat_tokenized[i])
    x_test_mul_repeat_normal_2.append(x_test_mul_repeat_normal[i])
    y_test_mul_repeat_2.append(y_mul_repeat_test[i])

print(empty_indices_mul_repeat_train)
print(len(empty_indices_mul_repeat_train))
print(len(x_train_mul_repeat_tokenized))
print(len(x_train_mul_repeat_tokenized_2))
print(len(x_train_mul_repeat_normal_2))
print(len(y_train_mul_repeat_2))

print(empty_indices_mul_repeat_val)
print(len(empty_indices_mul_repeat_val))
print(len(x_val_mul_repeat_tokenized))
print(len(x_val_mul_repeat_tokenized_2))
print(len(x_val_mul_repeat_normal_2))
print(len(y_val_mul_repeat_2))

print(empty_indices_mul_repeat_test)
print(len(empty_indices_mul_repeat_test))
print(len(x_test_mul_repeat_tokenized))
print(len(x_test_mul_repeat_tokenized_2))
print(len(x_test_mul_repeat_normal_2))
print(len(y_test_mul_repeat_2))

# Saving the objects:
with open('/content/drive/MyDrive/2021_NLU/data/multi_emoji/mul_repeat_emojis.pkl', 'wb') as f:  # Python 3: open(..., 'wb')
    pickle.dump([x_train_mul_repeat_tokenized, x_train_mul_repeat_normal, x_val_mul_repeat_tokenized, x_val_mul_repeat_normal, x_test_mul_repeat_tokenized, x_test_mul_repeat_normal, empty_indices_mul_repeat_train, x_train_mul_repeat_tokenized_2, \
                 x_train_mul_repeat_normal_2, empty_indices_mul_repeat_val, x_val_mul_repeat_tokenized_2, x_val_mul_repeat_normal_2, y_mul_repeat_train, y_mul_repeat_val, y_train_mul_repeat_2, y_val_mul_repeat_2, y_mul_repeat_test, y_test_mul_repeat_2,\
                 empty_indices_mul_repeat_test, x_test_mul_repeat_normal_2, x_test_mul_repeat_tokenized_2], f)

with open('/content/drive/MyDrive/2021_NLU/data/multi_emoji/mul_repeat_emojis.pkl', 'rb') as f:  
    x_train_mul_repeat_tokenized, x_train_mul_repeat_normal, x_val_mul_repeat_tokenized, x_val_mul_repeat_normal, x_test_mul_repeat_tokenized, x_test_mul_repeat_normal, empty_indices_mul_repeat_train, x_train_mul_repeat_tokenized_2, \
                 x_train_mul_repeat_normal_2, empty_indices_mul_repeat_val, x_val_mul_repeat_tokenized_2, x_val_mul_repeat_normal_2, y_mul_repeat_train, y_mul_repeat_val, y_train_mul_repeat_2, y_val_mul_repeat_2, y_mul_repeat_test, y_test_mul_repeat_2,\
                 empty_indices_mul_repeat_test, x_test_mul_repeat_normal_2, x_test_mul_repeat_tokenized_2 = pickle.load(f)

X_train_mul_repeat_w2vec = convert_word2vec(word2vec, x_train_mul_repeat_tokenized_2, strategy='mean')
X_val_mul_repeat_w2vec = convert_word2vec(word2vec, x_val_mul_repeat_tokenized_2, strategy='mean')
X_test_mul_repeat_w2vec = convert_word2vec(word2vec, x_test_mul_repeat_tokenized_2, strategy='mean')

# list(df_train['emoji_sentence'][2])
df_mul_repeat_train['emoji_sentence_list'] = df_mul_repeat_train['emoji_sentence'].apply(lambda x: list(x))
emoji_corpus_mul_repeat_train = df_mul_repeat_train['emoji_sentence_list'].values

df_mul_repeat_val['emoji_sentence_list'] = df_mul_repeat_val['emoji_sentence'].apply(lambda x: list(x))
emoji_corpus_mul_repeat_val = df_mul_repeat_val['emoji_sentence_list'].values

df_mul_repeat_test['emoji_sentence_list'] = df_mul_repeat_test['emoji_sentence'].apply(lambda x: list(x))
emoji_corpus_mul_repeat_test = df_mul_repeat_test['emoji_sentence_list'].values

X_train_mul_repeat_e2vec = convert_emoji2vec(e2v, emoji_corpus_mul_repeat_train, strategy='mean')
X_val_mul_repeat_e2vec = convert_emoji2vec(e2v, emoji_corpus_mul_repeat_val, strategy='mean')
X_test_mul_repeat_e2vec = convert_emoji2vec(e2v, emoji_corpus_mul_repeat_test, strategy='mean')

print(len(X_train_mul_repeat_w2vec))
print(len(X_train_mul_repeat_w2vec[4]))

print(len(X_val_mul_repeat_w2vec))
print(len(X_val_mul_repeat_w2vec[4]))

print(len(X_test_mul_repeat_w2vec))
print(len(X_test_mul_repeat_w2vec[4]))

print(len(X_train_mul_repeat_e2vec))
print(len(X_train_mul_repeat_e2vec[4]))

print(len(X_val_mul_repeat_e2vec))
print(len(X_val_mul_repeat_e2vec[4]))

print(len(X_test_mul_repeat_e2vec))
print(len(X_test_mul_repeat_e2vec[4]))

"""#### Averaged Vectors"""

X_train_mul_repeat_vec = (np.array(X_train_mul_repeat_w2vec) + np.array(X_train_mul_repeat_e2vec))/2
X_val_mul_repeat_vec = (np.array(X_val_mul_repeat_w2vec) + np.array(X_val_mul_repeat_e2vec))/2
X_test_mul_repeat_vec = (np.array(X_test_mul_repeat_w2vec) + np.array(X_test_mul_repeat_e2vec))/2

# Saving the objects:
with open('/content/drive/MyDrive/2021_NLU/data/multi_emoji/mul_repeat_emojis_vec.pkl', 'wb') as f:  # Python 3: open(..., 'wb')
    pickle.dump([X_train_mul_repeat_vec, X_val_mul_repeat_vec, X_test_mul_repeat_vec], f)

with open('/content/drive/MyDrive/2021_NLU/data/multi_emoji/mul_repeat_emojis_vec.pkl', 'rb') as f:  
    X_train_mul_repeat_vec, X_val_mul_repeat_vec, X_test_mul_repeat_vec = pickle.load(f)

"""#### Concatenated Vectors"""

X_train_mul_repeat_vec_concat = np.concatenate((X_train_mul_repeat_w2vec, X_train_mul_repeat_e2vec), axis=1) 
X_val_mul_repeat_vec_concat  = np.concatenate((X_val_mul_repeat_w2vec, X_val_mul_repeat_e2vec), axis=1) 
X_test_mul_repeat_vec_concat  = np.concatenate((X_test_mul_repeat_w2vec, X_test_mul_repeat_e2vec), axis=1)

# Saving the objects:
with open('/content/drive/MyDrive/2021_NLU/data/multi_emoji/mul_repeat_emojis_vec.pkl', 'wb') as f:  # Python 3: open(..., 'wb')
    pickle.dump([X_train_mul_repeat_vec_concat, X_val_mul_repeat_vec_concat, X_test_mul_repeat_vec_concat], f)

with open('/content/drive/MyDrive/2021_NLU/data/multi_emoji/mul_repeat_emojis_vec.pkl', 'rb') as f:  
    X_train_mul_repeat_vec_concat, X_val_mul_repeat_vec_concat, X_test_mul_repeat_vec_concat = pickle.load(f)

"""## MULTI EMOJIS WITH NO REPEATS - Download, Preprocess, Create Vectors"""

df_mul_train = pd.read_csv('/content/drive/MyDrive/2021_NLU/data/no_repeats/emoji_nsp_dataset_no_repeats_train.csv')
df_mul_val = pd.read_csv('/content/drive/MyDrive/2021_NLU/data/no_repeats/emoji_nsp_dataset_no_repeats_valid.csv')
df_mul_test = pd.read_csv('/content/drive/MyDrive/2021_NLU/data/no_repeats/emoji_nsp_dataset_no_repeats_test.csv')

df_mul_train.head()

print(len(df_mul_train))
print(len(df_mul_val))
print(len(df_mul_test))

df_mul_train = df_mul_train.dropna().drop_duplicates()
df_mul_val = df_mul_val.dropna().drop_duplicates()
df_mul_test = df_mul_test.dropna().drop_duplicates()

print(len(df_mul_train))
print(len(df_mul_val))
print(len(df_mul_test))

# sample_tokenized, sample_normal = clean_text(sample)
x_train_mul_tokenized, x_train_mul_normal = clean_text(df_mul_train['tweets'].values)
x_val_mul_tokenized, x_val_mul_normal = clean_text(df_mul_val['tweets'].values)
x_test_mul_tokenized, x_test_mul_normal = clean_text(df_mul_test['tweets'].values)

y_mul_train = df_mul_train['follows?'].values
y_mul_val = df_mul_val['follows?'].values
y_mul_test = df_mul_test['follows?'].values

#remove empty values for train
y_train_mul_2 = []
x_train_mul_tokenized_2 = []
x_train_mul_normal_2 = []
empty_indices_mul_train = []

for i in range(len(x_train_mul_tokenized)):
  if len(x_train_mul_tokenized[i])==0:
    empty_indices_mul_train.append(i)
  else:
    x_train_mul_tokenized_2.append(x_train_mul_tokenized[i])
    x_train_mul_normal_2.append(x_train_mul_normal[i])
    y_train_mul_2.append(y_mul_train[i])

#remove empty values for val
y_val_mul_2 = []
x_val_mul_tokenized_2 = []
x_val_mul_normal_2 = []
empty_indices_mul_val = []

for i in range(len(x_val_mul_tokenized)):
  if len(x_val_mul_tokenized[i])==0:
    empty_indices_mul_val.append(i)
  else:
    x_val_mul_tokenized_2.append(x_val_mul_tokenized[i])
    x_val_mul_normal_2.append(x_val_mul_normal[i])
    y_val_mul_2.append(y_mul_val[i])

#remove empty values for test
y_test_mul_2 = []
x_test_mul_tokenized_2 = []
x_test_mul_normal_2 = []
empty_indices_mul_test = []

for i in range(len(x_test_mul_tokenized)):
  if len(x_test_mul_tokenized[i])==0:
    empty_indices_mul_test.append(i)
  else:
    x_test_mul_tokenized_2.append(x_test_mul_tokenized[i])
    x_test_mul_normal_2.append(x_test_mul_normal[i])
    y_test_mul_2.append(y_mul_test[i])

print(empty_indices_mul_train)
print(len(empty_indices_mul_train))
print(len(x_train_mul_tokenized))
print(len(x_train_mul_tokenized_2))
print(len(x_train_mul_normal_2))
print(len(y_train_mul_2))

print(empty_indices_mul_val)
print(len(empty_indices_mul_val))
print(len(x_val_mul_tokenized))
print(len(x_val_mul_tokenized_2))
print(len(x_val_mul_normal_2))
print(len(y_val_mul_2))

print(empty_indices_mul_test)
print(len(empty_indices_mul_test))
print(len(x_test_mul_tokenized))
print(len(x_test_mul_tokenized_2))
print(len(x_test_mul_normal_2))
print(len(y_test_mul_2))

# Saving the objects:
with open('/content/drive/MyDrive/2021_NLU/data/no_repeats/mul_emojis.pkl', 'wb') as f:  # Python 3: open(..., 'wb')
    pickle.dump([x_train_mul_tokenized, x_train_mul_normal, x_val_mul_tokenized, x_val_mul_normal, x_test_mul_tokenized, x_test_mul_normal, empty_indices_mul_train, x_train_mul_tokenized_2, \
                 x_train_mul_normal_2, empty_indices_mul_val, x_val_mul_tokenized_2, x_val_mul_normal_2, y_mul_train, y_mul_val, y_train_mul_2, y_val_mul_2, y_mul_test, y_test_mul_2,\
                 empty_indices_mul_test, x_test_mul_normal_2, x_test_mul_tokenized_2], f)

with open('/content/drive/MyDrive/2021_NLU/data/no_repeats/mul_emojis.pkl', 'rb') as f:  
    x_train_mul_tokenized, x_train_mul_normal, x_val_mul_tokenized, x_val_mul_normal, x_test_mul_tokenized, x_test_mul_normal, empty_indices_mul_train, x_train_mul_tokenized_2, \
                 x_train_mul_normal_2, empty_indices_mul_val, x_val_mul_tokenized_2, x_val_mul_normal_2, y_mul_train, y_mul_val, y_train_mul_2, y_val_mul_2, y_mul_test, y_test_mul_2,\
                 empty_indices_mul_test, x_test_mul_normal_2, x_test_mul_tokenized_2 = pickle.load(f)

X_train_mul_w2vec = convert_word2vec(word2vec, x_train_mul_tokenized_2, strategy='mean')
X_val_mul_w2vec = convert_word2vec(word2vec, x_val_mul_tokenized_2, strategy='mean')
X_test_mul_w2vec = convert_word2vec(word2vec, x_test_mul_tokenized_2, strategy='mean')

# list(df_train['emoji_sentence'][2])
df_mul_train['emoji_sentence_list'] = df_mul_train['emoji_sentence'].apply(lambda x: list(x))
emoji_corpus_mul_train = df_mul_train['emoji_sentence_list'].values

df_mul_val['emoji_sentence_list'] = df_mul_val['emoji_sentence'].apply(lambda x: list(x))
emoji_corpus_mul_val = df_mul_val['emoji_sentence_list'].values

df_mul_test['emoji_sentence_list'] = df_mul_test['emoji_sentence'].apply(lambda x: list(x))
emoji_corpus_mul_test = df_mul_test['emoji_sentence_list'].values

X_train_mul_e2vec = convert_emoji2vec(e2v, emoji_corpus_mul_train, strategy='mean')
X_val_mul_e2vec = convert_emoji2vec(e2v, emoji_corpus_mul_val, strategy='mean')
X_test_mul_e2vec = convert_emoji2vec(e2v, emoji_corpus_mul_test, strategy='mean')

print(len(X_train_mul_w2vec))
print(len(X_train_mul_w2vec[4]))

print(len(X_val_mul_w2vec))
print(len(X_val_mul_w2vec[4]))

print(len(X_test_mul_w2vec))
print(len(X_test_mul_w2vec[4]))

print(len(X_train_mul_e2vec))
print(len(X_train_mul_e2vec[4]))

print(len(X_val_mul_e2vec))
print(len(X_val_mul_e2vec[4]))

print(len(X_test_mul_e2vec))
print(len(X_test_mul_e2vec[4]))

"""#### Averaged Vectors"""

X_train_mul_vec = (np.array(X_train_mul_w2vec) + np.array(X_train_mul_e2vec))/2
X_val_mul_vec = (np.array(X_val_mul_w2vec) + np.array(X_val_mul_e2vec))/2
X_test_mul_vec = (np.array(X_test_mul_w2vec) + np.array(X_test_mul_e2vec))/2

# Saving the objects:
with open('/content/drive/MyDrive/2021_NLU/data/no_repeats/mul_emojis_vec.pkl', 'wb') as f:  # Python 3: open(..., 'wb')
    pickle.dump([X_train_mul_vec, X_val_mul_vec, X_test_mul_vec], f)

with open('/content/drive/MyDrive/2021_NLU/data/no_repeats/mul_emojis_vec.pkl', 'rb') as f:  
    X_train_mul_vec, X_val_mul_vec, X_test_mul_vec = pickle.load(f)

"""#### Concatenated Vectors"""

X_train_mul_vec_concat = np.concatenate((X_train_mul_w2vec, X_train_mul_e2vec), axis=1) 
X_val_mul_vec_concat  = np.concatenate((X_val_mul_w2vec, X_val_mul_e2vec), axis=1) 
X_test_mul_vec_concat  = np.concatenate((X_test_mul_w2vec, X_test_mul_e2vec), axis=1)

# Saving the objects:
with open('/content/drive/MyDrive/2021_NLU/data/no_repeats/mul_emojis_vec.pkl', 'wb') as f:  # Python 3: open(..., 'wb')
    pickle.dump([X_train_mul_vec_concat, X_val_mul_vec_concat, X_test_mul_vec_concat], f)

with open('/content/drive/MyDrive/2021_NLU/data/no_repeats/mul_emojis_vec.pkl', 'rb') as f:  
    X_train_mul_vec_concat, X_val_mul_vec_concat, X_test_mul_vec_concat = pickle.load(f)

"""## LogisticRegression Model"""

from sklearn.metrics import f1_score, accuracy_score, confusion_matrix
from sklearn.metrics import make_scorer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

def metric(y_true, y_pred):
    return f1_score(y_true, y_pred, average='weighted')

my_scorer = make_scorer(metric, greater_is_better=True)

def run_model(model, X_train, y_train, X_val, y_val):
  print('     ## Fitting')
  model.fit(X_train, y_train)
  print('     ## Predicting')
  preds = model.predict(X_val)
  print(' ## F1 Score (weighted) is -', f1_score(y_val, preds, average='weighted'))
  print(' ## Accuracy is -', accuracy_score(y_val, preds))
  print('      ')
  print('               Confusion Matrix')
  print(confusion_matrix(y_val, preds))

lr = LogisticRegression()

def run_model_gridsearch_lr(model, X_train, y_train, X_val, y_val):
  
  cv=5
  parameters = {'C': np.logspace(0, 4, 10),'penalty': ['l1', 'l2']}
  clf = GridSearchCV(model, parameters, cv=cv, n_jobs=-1, scoring=my_scorer, verbose=0)
  print(' ## Fitting GridSearch')
  clf.fit(X_train, y_train)

  print("tuned hpyerparameters :(best parameters) ",clf.best_params_)
  print("score :",clf.best_score_)

def scores(y_val, preds):
  print(' ## F1 Score (weighted) is -', f1_score(y_val, preds, average='weighted'))
  print(' ## Accuracy is -', accuracy_score(y_val, preds))
  print('      ')
  print('               Confusion Matrix')
  print(confusion_matrix(y_val, preds))



"""###**LR with FULL DATA**

#### With Averaged Vectors
"""

run_model(lr, X_train_vec, y_train, X_val_vec, y_val)

run_model_gridsearch_lr(lr, X_train_vec, y_train, X_val_vec, y_val)

best_lr_full = LogisticRegression(penalty = 'l2', C = 3593.813663804626)
run_model(best_lr_full, X_train_vec, y_train, X_val_vec, y_val)

"""**PREDICTION ON TEST (w Avg Vectors)**"""

pred_lr_full = best_lr_full.predict(X_test_vec)
pred_lr_full_prob = best_lr_full.predict_proba(X_test_vec)
scores(y_test, pred_lr_full)

pred_lr_full_prob #[prob for class0, prob for class1]

"""#### With Concantenated Vectors"""

run_model(lr, X_train_vec_concat, y_train, X_val_vec_concat, y_val)

run_model_gridsearch_lr(lr, X_train_vec_concat, y_train, X_val_vec_concat, y_val)

best_lr_full_concat = LogisticRegression(penalty = 'l2', C = 2.7825594022071245)
run_model(best_lr_full_concat, X_train_vec_concat, y_train, X_val_vec_concat, y_val)

"""**PREDICTION ON TEST (w Concat Vectors)**"""

pred_lr_full_concat = best_lr_full_concat.predict(X_test_vec_concat)
pred_lr_full_prob_concat = best_lr_full_concat.predict_proba(X_test_vec_concat)
scores(y_test, pred_lr_full_concat)

pred_lr_full_prob_concat #[prob for class0, prob for class1]

"""###**LR with SINGLE EMOJI**

#### With Averaged Vectors
"""

# FOR SINGLE EMOJI
run_model(lr, X_train_sing_vec, y_sing_train, X_val_sing_vec, y_sing_val)

run_model_gridsearch_lr(lr, X_train_sing_vec, y_sing_train, X_val_sing_vec, y_sing_val)

best_lr_sing = LogisticRegression(penalty = 'l2', C = 10000.0)
run_model(best_lr_sing, X_train_sing_vec, y_sing_train, X_val_sing_vec, y_sing_val)

"""**PREDICTION ON TEST (w Avg Vectors)**"""

pred_lr_sing = best_lr_sing.predict(X_test_sing_vec)
scores(y_sing_test, pred_lr_sing)

"""#### With Concatenated Vectors"""

run_model(lr, X_train_sing_vec_concat, y_sing_train, X_val_sing_vec_concat, y_sing_val)

run_model_gridsearch_lr(lr, X_train_sing_vec_concat, y_sing_train, X_val_sing_vec_concat, y_sing_val)

best_lr_sing_concat = LogisticRegression(penalty = 'l2', C = 21.544346900318832)
run_model(best_lr_sing_concat, X_train_sing_vec_concat, y_sing_train, X_val_sing_vec_concat, y_sing_val)

"""**PREDICTION ON TEST (w Concat Vectors)**"""

pred_lr_sing_concat = best_lr_sing_concat.predict(X_test_sing_vec_concat)
scores(y_sing_test, pred_lr_sing_concat)

"""###**MULTI EMOJIS WITH REPEATS**

#### With Averaged Vectors
"""

run_model(lr, X_train_mul_repeat_vec, y_mul_repeat_train, X_val_mul_repeat_vec, y_mul_repeat_val)

run_model_gridsearch_lr(lr, X_train_mul_repeat_vec, y_mul_repeat_train, X_val_mul_repeat_vec, y_mul_repeat_val)

best_lr_mul_repeat = LogisticRegression(penalty = 'l2', C = 2.7825594022071245)
run_model(best_lr_mul_repeat, X_train_mul_repeat_vec, y_mul_repeat_train, X_val_mul_repeat_vec, y_mul_repeat_val)

"""**PREDICTION ON TEST (w Avg Vectors)**"""

pred_lr_mul_repeat = best_lr_mul_repeat.predict(X_test_mul_repeat_vec)
scores(y_mul_repeat_test, pred_lr_mul_repeat)

"""#### With Concantenated Vectors"""

run_model(lr, X_train_mul_repeat_vec_concat, y_mul_repeat_train, X_val_mul_repeat_vec_concat, y_mul_repeat_val)

run_model_gridsearch_lr(lr, X_train_mul_repeat_vec_concat, y_mul_repeat_train, X_val_mul_repeat_vec_concat, y_mul_repeat_val)

best_lr_mul_repeat_concat = LogisticRegression(penalty = 'l2', C = 1.0)
run_model(best_lr_mul_repeat_concat, X_train_mul_repeat_vec_concat, y_mul_repeat_train, X_val_mul_repeat_vec_concat, y_mul_repeat_val)

"""**PREDICTION ON TEST (w Concat Vectors)**"""

pred_lr_mul_repeat_concat = best_lr_mul_repeat_concat.predict(X_test_mul_repeat_vec_concat)
scores(y_mul_repeat_test, pred_lr_mul_repeat_concat)

"""###**MULTI EMOJIS NO REPEATS**

#### With Averaged Vectors
"""

run_model(lr, X_train_mul_vec, y_mul_train, X_val_mul_vec, y_mul_val)

run_model_gridsearch_lr(lr, X_train_mul_vec, y_mul_train, X_val_mul_vec, y_mul_val)

best_lr_mul = LogisticRegression(penalty = 'l2', C = 7.742636826811269)
run_model(best_lr_mul, X_train_mul_vec, y_mul_train, X_val_mul_vec, y_mul_val)

"""**PREDICTION ON TEST (w Avg Vectors)**"""

pred_lr_mul = best_lr_mul.predict(X_test_mul_vec)
scores(y_mul_test, pred_lr_mul)

"""#### With Concantenated Vectors"""

run_model(lr, X_train_mul_vec_concat, y_mul_train, X_val_mul_vec_concat, y_mul_val)

run_model_gridsearch_lr(lr, X_train_mul_vec_concat, y_mul_train, X_val_mul_vec_concat, y_mul_val)

best_lr_mul_concat = LogisticRegression(penalty = 'l2', C = 166.81005372000593)
run_model(best_lr_mul_concat, X_train_mul_vec_concat, y_mul_train, X_val_mul_vec_concat, y_mul_val)

"""**PREDICTION ON TEST (w Concat Vectors)**"""

pred_lr_mul_concat = best_lr_mul_concat.predict(X_test_mul_vec_concat)
scores(y_mul_test, pred_lr_mul_concat)

"""##**SUMMARY PREDICTION ON TEST**

###**w Avg Vectors**
"""

#FULL DATA
print("FULL DATA")
print("-------------------")
pred_lr_full = best_lr_full.predict(X_test_vec)
scores(y_test, pred_lr_full)
print("==========================================")

#FULL EMOJIS WITH NO REPEATS
print(" ")
print("FULL NO REPEATS")
print("-------------------")
pred_lr_mul = best_lr_mul.predict(X_test_mul_vec)
scores(y_mul_test, pred_lr_mul)
print("==========================================")

#SINGLE EMOJI
print(" ")
print("SINGLE EMOJI ONLY")
print("-------------------")
pred_lr_sing = best_lr_sing.predict(X_test_sing_vec)
scores(y_sing_test, pred_lr_sing)
print("==========================================")

#MULTI EMOJIS WITH REPEATS
print(" ")
print("MULTI EMOJIS ONLY WITH REPEATS")
print("-------------------")
pred_lr_mul_repeat = best_lr_mul_repeat.predict(X_test_mul_repeat_vec)
scores(y_mul_repeat_test, pred_lr_mul_repeat)

"""###**w Concat Vectors**"""

#FULL DATA
print("FULL DATA CONCAT")
print("-------------------")
pred_lr_full_concat = best_lr_full_concat.predict(X_test_vec_concat)
scores(y_test, pred_lr_full_concat)
print("==========================================")

#FULL EMOJIS WITH NO REPEATS
print(" ")
print("FULL NO REPEATS CONCAT")
print("-------------------")
# pred_lr_mul = best_lr_mul.predict(X_test_mul_vec)
# scores(y_mul_test, pred_lr_mul)
pred_lr_mul_concat = best_lr_mul_concat.predict(X_test_mul_vec_concat)
scores(y_mul_test, pred_lr_mul_concat)

print("==========================================")

#SINGLE EMOJI
print(" ")
print("SINGLE EMOJI ONLY CONCAT")
print("-------------------")
pred_lr_sing_concat = best_lr_sing_concat.predict(X_test_sing_vec_concat)
scores(y_sing_test, pred_lr_sing_concat)
print("==========================================")

#MULTI EMOJIS WITH REPEATS
print(" ")
print("MULTI EMOJIS ONLY WITH REPEATS CONCAT")
print("-------------------")
pred_lr_mul_repeat_concat = best_lr_mul_repeat_concat.predict(X_test_mul_repeat_vec_concat)
scores(y_mul_repeat_test, pred_lr_mul_repeat_concat)



"""##**ERROR ANALYSIS FOR BEST RESULTS (FULL DATA w Avg Vectors)**"""

def evaluate_incorrect(y_test, preds, input_df, preds_prob):
  df_test = input_df.copy()
  df_test['prob_zero'] = preds_prob[:, 0]
  print(df_test.shape)
  eval_incorrect = df_test.index[y_test != preds]

  output_incorrect = df_test[df_test.index.isin(eval_incorrect)]
  # print(eval)
  # print(eval.shape)
  # print(y_test != preds)
  # print(y_test == preds)

  return output_incorrect

def evaluate_correct(y_test, preds, input_df, preds_prob):
  df_test = input_df.copy()
  df_test['prob_zero'] = preds_prob[:, 0]
  print(df_test.shape)

  eval_correct = df_test.index[y_test == preds]
  output_correct = df_test[df_test.index.isin(eval_correct)]
  # print(eval)
  # print(eval.shape)
  # print(y_test != preds)
  # print(y_test == preds)

  return output_correct

best_lr_full = LogisticRegression(penalty = 'l2', C = 3593.813663804626)
run_model(best_lr_full, X_train_vec, y_train, X_val_vec, y_val)

best_lr_full.predict_proba

pred_lr_full_prob = best_lr_full.predict_proba(X_test_vec)

pred_lr_full_prob[:, 0]

output_incorrect = evaluate_incorrect(y_test, pred_lr_full, df_test, pred_lr_full_prob)
output_incorrect

output_incorrect.to_csv("/content/drive/MyDrive/2021_NLU/data/full_data/output_incorrect.csv", index=False)

incorrect = pd.read_csv("/content/drive/MyDrive/2021_NLU/data/full_data/output_incorrect.csv")

print(incorrect.columns)
print(incorrect.shape)
print(len(x_test_tokenized))

# below df is original + model predicting prob of zero
# original says 'follows=0', which means it doesn't follow 
# but the below shows 'incorrect' prediction
# and model predicted it doesn't follow with a 0.139464 prob 
# which means the model predicted it "follows" with a 0.860536 (1-0.139464) prob  
incorrect[incorrect['follows?'] == 0].sort_values(by='prob_zero').head(10)

4 + 3 + 3 + 5 +

incorrect[incorrect['follows?'] == 1].sort_values(by='prob_zero', ascending = False).head(10)





output_correct = evaluate_correct(y_test, pred_lr_full, df_test, pred_lr_full_prob)

output_correct.to_csv("/content/drive/MyDrive/2021_NLU/data/full_data/output_correct.csv", index=False)

output_correct

output_correct[output_correct['prob_zero']  > 0.7]

correct_prob = output_correct[output_correct['follows?'] == 0].prob_zero

plt.hist(correct_prob)

incorrect_prob = 1-output_correct[output_correct['follows?'] == 1].prob_zero

plt.hist(incorrect_prob)

output_correct['emoji_sentence_len'] = output_correct['emoji_sentence'].apply(lambda x: len(x))

output_correct[output_correct['emoji_sentence_len'] == 1]

output_incorrect['emoji_sentence_len'] = output_incorrect['emoji_sentence'].apply(lambda x: len(x))

output_incorrect[output_incorrect['emoji_sentence_len'] == 1]

df_test['follows?'].value_counts()